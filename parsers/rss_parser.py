# –≠—Ç–æ—Ç —Ñ–∞–π–ª —É—Å—Ç–∞—Ä–µ–ª. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ parsers/unified_parser.py
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–π —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
from parsers.unified_parser import (
    UnifiedParser, 
    parse_source, 
    parse_all_sources,
    get_sync_parser,
    get_async_parser
)

import logging
from typing import Dict, List, Optional

from services.categories import get_all_sources

logger = logging.getLogger("parsers.rss")

HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; NewsBot/1.0; +https://example.com)"}


def normalize_date(date_str: str | None):
    """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç UTC datetime –∏–ª–∏ None."""
    if not date_str:
        return None
    try:
        dt = dtp.parse(date_str)
        if not dt.tzinfo:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: {date_str} ({e})")
        return None


def load_sources(
    category: Optional[str] = None, subcategory: Optional[str] = None
) -> Dict[str, Dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ services/categories."""
    all_sources = get_all_sources()
    urls: Dict[str, Dict] = {}

    for cat, subcat, name, url in all_sources:
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã
        if category and cat != category:
            continue
        if subcategory and subcat != subcategory:
            continue

        urls[name] = {"name": name, "url": url, "category": cat, "subcategory": subcat}

    return urls


def parse_source(url: str, category: str, subcategory: str, source_name: str) -> List[Dict]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ RSS.

    Args:
        url: URL RSS —Ñ–∏–¥–∞
        category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–æ–≤–æ—Å—Ç–∏
        subcategory: –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–æ–≤–æ—Å—Ç–∏
        source_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞

    Returns:
        List[Dict]: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–ª—è–º–∏ category –∏ subcategory
    """
    try:
        feed = fetch_feed(url)
        if not feed or not feed.entries:
            logger.warning(f"–ü—É—Å—Ç–æ–π —Ñ–∏–¥: {source_name} ({url})")
            return []

        news_items = []
        for entry in feed.entries:
            try:
                # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
                title = clean_text(entry.get("title", ""))
                content = clean_text(entry.get("summary", ""))

                if not title:
                    continue

                # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã
                published_at = normalize_date(entry.get("published", entry.get("updated")))
                if not published_at:
                    published_at = normalize_date(str(entry.get("published_parsed")))

                # –°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID
                uid = hashlib.md5(f"{entry.get('link', '')}{title}".encode()).hexdigest()

                news_item = {
                    "uid": uid,
                    "title": title,
                    "content": content,
                    "link": entry.get("link", ""),  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: url -> link
                    "source": source_name,
                    "category": category,
                    "subcategory": subcategory,
                    "published_at": published_at.isoformat() if published_at else None,
                }

                news_items.append(news_item)

            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø–∏—Å–∏ –∏–∑ {source_name}: {e}")
                continue

        logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ {source_name}: {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        return news_items

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source_name}: {e}")
        return []


def fetch_feed(url: str):
    """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç —Ñ–∏–¥ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç MIME-—Ç–∏–ø (—á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å text/html)."""
    try:
        resp = requests.get(url, timeout=10, headers=HEADERS)
        ctype = resp.headers.get("Content-Type", "")
        if "xml" not in ctype and "rss" not in ctype:
            raise ValueError(f"Invalid content-type {ctype} for {url}")
        return feedparser.parse(resp.content)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
        return None


def fetch_rss(urls: Dict[str, Dict], per_source_limit: Optional[int] = None) -> List[Dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π subcategory."""
    news_items = []
    seen = set()

    for meta in urls.values():
        logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫: {meta['name']} ({meta['url']})")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é parse_source
        source_items = parse_source(
            url=meta["url"],
            category=meta.get("category", ""),
            subcategory=meta.get("subcategory", ""),
            source_name=meta["name"],
        )

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–∏–º–∏—Ç –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        if per_source_limit:
            source_items = source_items[:per_source_limit]

        # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for item in source_items:
            uid = item["uid"]
            if uid not in seen:
                seen.add(uid)
                news_items.append(item)

        logger.info(f"‚úÖ {meta['name']}: {len(source_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–æ–±–∞–≤–ª–µ–Ω–æ")

    return news_items
