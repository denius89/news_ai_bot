import hashlib
import logging
from datetime import timezone
from pathlib import Path

import requests
import feedparser
import yaml
from dateutil import parser as dtp

from utils.clean_text import clean_text  # –≤—ã–Ω–µ—Å–µ–Ω–æ –æ—Ç–¥–µ–ª—å–Ω–æ

logger = logging.getLogger("parsers.rss")

CONFIG_PATH = Path(__file__).resolve().parent.parent / "config" / "sources.yaml"

HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; NewsBot/1.0; +https://example.com)"}


def normalize_date(date_str: str | None):
    """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç UTC datetime –∏–ª–∏ None."""
    if not date_str:
        return None
    try:
        dt = dtp.parse(date_str)
        if not dt.tzinfo:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: {date_str} ({e})")
        return None


def load_sources(category: str | None = None) -> dict[str, dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ sources.yaml."""
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        sources = yaml.safe_load(f)

    urls: dict[str, dict] = {}
    if category:
        if category not in sources:
            raise ValueError(f"–ù–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '{category}' –≤ sources.yaml")
        group = sources[category]
        for item in group:
            urls[item["name"]] = {**item, "category": category}
    else:
        for cat, group in sources.items():
            for item in group:
                urls[item["name"]] = {**item, "category": cat}

    return urls


def fetch_feed(url: str):
    """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç —Ñ–∏–¥ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç MIME-—Ç–∏–ø (—á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å text/html)."""
    try:
        resp = requests.get(url, timeout=10, headers=HEADERS)
        ctype = resp.headers.get("Content-Type", "")
        if "xml" not in ctype and "rss" not in ctype:
            raise ValueError(f"Invalid content-type {ctype} for {url}")
        return feedparser.parse(resp.content)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
        return None


def fetch_rss(urls: dict[str, dict], per_source_limit: int | None = None) -> list[dict]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
    news_items = []
    seen = set()

    for meta in urls.values():
        logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫: {meta['name']} ({meta['url']})")
        feed = fetch_feed(meta["url"])
        if not feed or feed.bozo:
            logger.error(
                f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {meta['url']}: {getattr(feed, 'bozo_exception', '')}"
            )
            continue

        items_before = len(news_items)
        for entry in feed.entries[:per_source_limit]:
            url = entry.get("link") or ""
            title = clean_text(entry.get("title", ""))
            summary = clean_text(entry.get("summary") or entry.get("description") or "")
            published = normalize_date(entry.get("published") or entry.get("updated"))

            uid = hashlib.sha256(f"{url}|{title}".encode()).hexdigest()
            if uid in seen:
                continue
            seen.add(uid)

            news_items.append(
                {
                    "uid": uid,
                    "title": title,
                    "url": url,
                    "summary": summary or title,
                    "published_at": published,
                    "source": meta["name"],
                    "category": meta["category"],
                }
            )

        items_added = len(news_items) - items_before
        if items_added > 0:
            logger.info(f"‚úÖ {meta['name']}: {items_added} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–æ–±–∞–≤–ª–µ–Ω–æ")
        else:
            logger.warning(f"‚ö†Ô∏è {meta['name']}: –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    return news_items
