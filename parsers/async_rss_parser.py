"""
–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è PulseAI.
"""

import asyncio
import hashlib
import logging
import sys
import aiohttp
import feedparser
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional

from dateutil import parser as dtp

sys.path.append(str(Path(__file__).parent.parent))

from utils.clean_text import clean_text  # noqa: E402
from services.categories import get_all_sources  # noqa: E402
from database.service import async_upsert_news, get_async_service  # noqa: E402

logger = logging.getLogger("parsers.async_rss")

HEADERS = {"User-Agent": "Mozilla/5.0 (compatible; NewsBot/1.0; +https://example.com)"}


def normalize_date(date_str: str | None):
    """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç UTC datetime –∏–ª–∏ None."""
    if not date_str:
        return None
    try:
        dt = dtp.parse(date_str)
        if not dt.tzinfo:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: {date_str} ({e})")
        return None


async def fetch_feed_async(session: aiohttp.ClientSession, url: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç RSS —Ñ–∏–¥."""
    try:
        async with session.get(
            url, headers=HEADERS, timeout=aiohttp.ClientTimeout(total=10)
        ) as response:
            content_type = response.headers.get('content-type', '').lower()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ XML/RSS, –∞ –Ω–µ HTML
            if 'xml' in content_type or 'rss' in content_type or 'atom' in content_type:
                content = await response.text()
                return feedparser.parse(content)
            else:
                logger.warning(f"–ù–µ–≤–µ—Ä–Ω—ã–π content-type –¥–ª—è {url}: {content_type}")
                return None

    except asyncio.TimeoutError:
        logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}")
        return None
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
        return None


async def parse_source_async(
    session: aiohttp.ClientSession, url: str, category: str, subcategory: str, source_name: str
) -> List[Dict]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω RSS –∏—Å—Ç–æ—á–Ω–∏–∫."""
    try:
        feed = await fetch_feed_async(session, url)
        if not feed or not feed.entries:
            logger.warning(f"–ü—É—Å—Ç–æ–π —Ñ–∏–¥: {source_name} ({url})")
            return []

        news_items = []
        for entry in feed.entries:
            try:
                # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
                title = clean_text(entry.get("title", ""))
                content = clean_text(entry.get("summary", ""))

                if not title:
                    continue

                # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã
                published_at = normalize_date(entry.get("published", entry.get("updated")))
                if not published_at:
                    published_at = normalize_date(str(entry.get("published_parsed")))

                # –°–æ–∑–¥–∞–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID
                uid = hashlib.md5(f"{entry.get('link', '')}{title}".encode()).hexdigest()

                news_item = {
                    "uid": uid,
                    "title": title,
                    "content": content,
                    "link": entry.get("link", ""),
                    "source": source_name,
                    "category": category,
                    "subcategory": subcategory,
                    "published_at": published_at.isoformat() if published_at else None,
                    "created_at": datetime.now(timezone.utc).isoformat(),
                }

                news_items.append(news_item)

            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø–∏—Å–∏ –∏–∑ {source_name}: {e}")
                continue

        logger.info(f"‚úÖ Async –ø–∞—Ä—Å–∏–Ω–≥ {source_name}: {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        return news_items

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ async –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source_name}: {e}")
        return []


async def parse_all_sources_async(per_source_limit: Optional[int] = None) -> List[Dict]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–∞—Ä—Å–∏—Ç –≤—Å–µ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ."""
    all_sources = get_all_sources()

    if not all_sources:
        logger.warning("–ù–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
        return []

    logger.info(f"üîÑ Async –ø–∞—Ä—Å–∏–Ω–≥ {len(all_sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")

    # –°–æ–∑–¥–∞–µ–º HTTP —Å–µ—Å—Å–∏—é
    async with aiohttp.ClientSession() as session:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        tasks = []
        for cat, subcat, name, url in all_sources:
            task = parse_source_async(session, url, cat, subcat, name)
            tasks.append(task)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        all_news = []
        seen = set()

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ {i}: {result}")
                continue

            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–∏–º–∏—Ç –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
            if per_source_limit and len(result) > per_source_limit:
                result = result[:per_source_limit]

            # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for item in result:
                uid = item["uid"]
                if uid not in seen:
                    seen.add(uid)
                    all_news.append(item)

    logger.info(f"‚úÖ Async –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(all_news)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
    return all_news


async def async_parse_and_save():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö."""
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
    async_service = get_async_service()
    await async_service._init_async_client()
    if not async_service.async_client:
        logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Async Supabase")
        return 0

    # –ü–∞—Ä—Å–∏–º –Ω–æ–≤–æ—Å—Ç–∏
    news_items = await parse_all_sources_async(per_source_limit=5)

    if not news_items:
        logger.warning("–ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return 0

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    saved_count = await async_upsert_news(news_items)

    logger.info(f"üéâ Async –ø–∞—Ä—Å–∏–Ω–≥ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω—ã: {saved_count} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return saved_count


async def compare_sync_vs_async():
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞."""
    import time

    # –¢–µ—Å—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
    logger.info("üîÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥...")
    start_time = time.time()
    async_news = await parse_all_sources_async(per_source_limit=3)
    async_time = time.time() - start_time

    logger.info("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞:")
    logger.info(f"  Async: {len(async_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ {async_time:.2f}—Å")


if __name__ == "__main__":

    async def main():
        # –¢–µ—Å—Ç –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
        await async_parse_and_save()

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        await compare_sync_vs_async()

    asyncio.run(main())
