"""
Module: parsers.advanced_parser
Purpose: Advanced News Parser with AI-powered content extraction
Location: parsers/advanced_parser.py

Description:
    –£–º–Ω—ã–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ª—É—á—à–∏–µ —Ä–µ—à–µ–Ω–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (RSS/HTML/API) –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –∫–∞—Å–∫–∞–¥–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏–∑–≤–ª–µ—á–µ–Ω–∏—è.

    ‚úÖ –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø: –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è production –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

Key Features:
    - –ö–∞—Å–∫–∞–¥–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (4 —É—Ä–æ–≤–Ω—è fallback)
    - AI-powered —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (RSS, HTML, API)
    - Async/await –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    - Retry logic —Å exponential backoff

Extraction Strategy (Priority Order):
    1. news-please (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1) - –ª—É—á—à–∏–π –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–∞–π—Ç–æ–≤
    2. Fundus (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2) - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π
    3. trafilatura (fallback) - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π extractor
    4. AutoScraper (–ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å) - –∫–æ–≥–¥–∞ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω–æ–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç

AI Processing:
    - evaluate_importance(): –û—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ (0.1-1.0)
    - evaluate_credibility(): –û—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏

Dependencies:
    External:
        - news-please: News content extraction
        - fundus: News-specific extraction
        - trafilatura: Universal content extraction
        - autoscraper: Fallback extraction
        - httpx: Async HTTP client
    Internal:
        - database.service: Database operations (modern approach)
        - ai_modules.importance: Importance scoring
        - ai_modules.credibility: Credibility scoring
        - config.data.sources: Source configuration

Usage Example:
    ```python
    from parsers.advanced_parser import AdvancedParser

    # Async context manager (recommended)
    async with AdvancedParser() as parser:
        await parser.run()

    # Manual usage
    parser = AdvancedParser()
    await parser._init_session()
    await parser._load_sources_config()
    await parser.run()
    ```

Configuration:
    Sources configuration in `config/data/sources.yaml`:
    ```yaml
    sources:
      - name: "TechCrunch"
        url: "https://techcrunch.com/feed/"
        type: "rss"
        category: "tech"
        enabled: true
    ```

Performance:
    - Async processing –¥–ª—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - Batch processing –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    - Connection pooling —á–µ—Ä–µ–∑ httpx
    - Retry logic –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏

Notes:
    - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π database.service (–Ω–µ legacy db_models)
    - –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç async context manager
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    - –õ–æ–≥–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ—Ü–µ—Å—Å–µ
    - TODO (Future): –î–æ–±–∞–≤–∏—Ç—å metrics –∏ monitoring (–∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞ Week 3)

Author: PulseAI Team
Last Updated: October 2025
"""

import asyncio
import logging
import time
from typing import Dict, List, Optional, Tuple, Any
import yaml
from pathlib import Path

import aiohttp
import feedparser
from newsplease import NewsPlease
import trafilatura
from autoscraper import AutoScraper

from database.service import get_async_service
from ai_modules.importance import evaluate_importance
from ai_modules.credibility import evaluate_credibility
from utils.text.clean_text import clean_text

logger = logging.getLogger(__name__)


class AdvancedParser:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å AI-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π."""

    def __init__(self, max_concurrent: int = 10, min_importance: float = 0.3):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞.

        Args:
            max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            min_importance: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏
        """
        self.max_concurrent = max_concurrent
        self.min_importance = min_importance
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session: Optional[aiohttp.ClientSession] = None
        self.sources_config: Dict = {}

    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥."""
        await self._init_session()
        await self._load_sources_config()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥."""
        await self._close_session()

    async def _init_session(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è HTTP —Å–µ—Å—Å–∏–∏."""
        # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π timeout –¥–ª—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (WEF, World Bank, IMF)
        timeout = aiohttp.ClientTimeout(total=60, connect=15, sock_read=45)
        connector = aiohttp.TCPConnector(limit=100, limit_per_host=10)

        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            ),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }

        self.session = aiohttp.ClientSession(timeout=timeout, connector=connector, headers=headers)

    async def _close_session(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ HTTP —Å–µ—Å—Å–∏–∏."""
        if self.session:
            await self.session.close()

    async def _load_sources_config(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ YAML."""
        config_path = Path("config/data/sources.yaml")
        if not config_path.exists():
            logger.error("–§–∞–π–ª config/sources.yaml –Ω–µ –Ω–∞–π–¥–µ–Ω")
            self.sources_config = {}
            return

        try:
            with open(config_path, "r", encoding="utf-8") as f:
                self.sources_config = yaml.safe_load(f)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ {config_path}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            self.sources_config = {}

    def _get_all_sources(self) -> List[Tuple[str, str, str, str]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.

        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (category, subcategory, name, url)
        """
        sources = []

        if not self.sources_config:
            return sources

        for category, category_data in self.sources_config.items():
            if not isinstance(category_data, dict):
                continue

            for subcategory, subcategory_data in category_data.items():
                if not isinstance(subcategory_data, dict):
                    continue

                sources_list = subcategory_data.get("sources", [])
                for source in sources_list:
                    if isinstance(source, dict):
                        name = source.get("name", "")
                        url = source.get("url", "")
                        if name and url:
                            sources.append((category, subcategory, name, url))
                    elif isinstance(source, str):
                        # –ü—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç: "Name: URL"
                        if ":" in source:
                            name, url = source.split(":", 1)
                            sources.append((category, subcategory, name.strip(), url.strip()))

        return sources

    async def _fetch_content(self, url: str) -> Tuple[bool, str, Optional[bytes]]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ URL.

        Args:
            url: URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏

        Returns:
            –ö–æ—Ä—Ç–µ–∂ (success, content_type, content_bytes)
        """
        try:
            async with self.session.get(url, allow_redirects=True) as response:
                if response.status == 200:
                    content_bytes = await response.read()
                    content_type = response.headers.get("content-type", "").lower()
                    return True, content_type, content_bytes
                else:
                    logger.warning(f"HTTP {response.status} –¥–ª—è {url}")
                    return False, None, None

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return False, None, None

    async def _fetch_content_with_retry(self, url: str, max_retries: int = 3) -> Tuple[bool, str, Optional[bytes]]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å retry –∏ exponential backoff.

        Args:
            url: URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3)

        Returns:
            –ö–æ—Ä—Ç–µ–∂ (success, content_type, content_bytes)
        """
        for attempt in range(max_retries):
            try:
                success, content_type, content = await self._fetch_content(url)

                if success:
                    if attempt > 0:
                        logger.info(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —Å –ø–æ–ø—ã—Ç–∫–∏ {attempt + 1}: {url}")
                    return success, content_type, content

                # –ï—Å–ª–∏ –Ω–µ —É—Å–ø–µ—à–Ω–æ –∏ –µ—Å—Ç—å –µ—â–µ –ø–æ–ø—ã—Ç–∫–∏ - –∂–¥–µ–º
                if attempt < max_retries - 1:
                    wait_time = 2**attempt  # 1s, 2s, 4s
                    logger.debug(f"–ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {wait_time}s (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 2}/{max_retries}): {url}")
                    await asyncio.sleep(wait_time)

            except Exception as e:
                if attempt == max_retries - 1:
                    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {url} - {e}")
                    return False, None, None

                wait_time = 2**attempt
                logger.debug(f"–û—à–∏–±–∫–∞, –ø–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {wait_time}s: {url} - {e}")
                await asyncio.sleep(wait_time)

        return False, None, None

    def _extract_with_newsplease(self, url: str, content: bytes) -> Optional[Dict[str, str]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é news-please.

        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            content: HTML –∫–æ–Ω—Ç–µ–Ω—Ç

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å title –∏ maintext –∏–ª–∏ None
        """
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º from_url (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± –¥–ª—è NewsPlease)
        try:
            article = NewsPlease.from_url(url, timeout=30)
            if article and article.maintext:
                return {
                    "title": clean_text(article.title or ""),
                    "maintext": clean_text(article.maintext),
                    "method": "newsplease_url",
                }
        except Exception as e:
            logger.debug(f"newsplease from_url failed for {url}: {e}")

        # Fallback –Ω–∞ from_html –µ—Å–ª–∏ URL –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
        try:
            content_str = content.decode("utf-8", errors="ignore")
            article = NewsPlease.from_html(content_str, url=url)

            if article and article.maintext:
                return {
                    "title": clean_text(article.title or ""),
                    "maintext": clean_text(article.maintext),
                    "method": "newsplease_html",
                }
        except Exception as e:
            logger.debug(f"newsplease from_html failed for {url}: {e}")

        return None

    def _extract_with_trafilatura(self, url: str, content: bytes) -> Optional[Dict[str, str]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é trafilatura.

        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            content: HTML –∫–æ–Ω—Ç–µ–Ω—Ç

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å title –∏ maintext –∏–ª–∏ None
        """
        try:
            html_content = content.decode("utf-8", errors="ignore")

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
            maintext = trafilatura.extract(html_content)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = trafilatura.extract_metadata(html_content).get("title", "")

            if maintext and len(maintext.strip()) > 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
                return {
                    "title": clean_text(title) if title else "Untitled",
                    "maintext": clean_text(maintext),
                    "method": "trafilatura",
                }

        except Exception as e:
            logger.debug(f"trafilatura failed for {url}: {e}")

        return None

    def _extract_with_autoscraper(self, url: str, content: bytes) -> Optional[Dict[str, str]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é AutoScraper.

        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            content: HTML –∫–æ–Ω—Ç–µ–Ω—Ç

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å title –∏ maintext –∏–ª–∏ None
        """
        try:
            html_content = content.decode("utf-8", errors="ignore")

            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é AutoScraper
            scraper = AutoScraper()

            # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            rules = [{"title": ["h1", "h2", "title"]}, {"content": ["p", "div", "article"]}]

            for rule in rules:
                try:
                    result = scraper.build(html_content, rule)
                    if result:
                        title = ""
                        maintext = ""

                        if "title" in result and result["title"]:
                            title = (
                                str(result["title"][0]) if isinstance(result["title"], list) else str(result["title"])
                            )

                        if "content" in result and result["content"]:
                            if isinstance(result["content"], list):
                                # –ü–µ—Ä–≤—ã–µ 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                                maintext = " ".join([str(item) for item in result["content"][:5]])
                            else:
                                maintext = str(result["content"])

                        if maintext and len(maintext.strip()) > 100:
                            return {
                                "title": clean_text(title) if title else "Untitled",
                                "maintext": clean_text(maintext),
                                "method": "autoscraper",
                            }

                except Exception as e:
                    logger.debug(f"AutoScraper rule failed: {e}")
                    continue

        except Exception as e:
            logger.debug(f"autoscraper failed for {url}: {e}")

        return None

    def _extract_content_cascade(self, url: str, content: bytes) -> Optional[Dict[str, str]]:
        """
        –ö–∞—Å–∫–∞–¥–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏.

        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            content: HTML –∫–æ–Ω—Ç–µ–Ω—Ç

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å title –∏ maintext –∏–ª–∏ None
        """
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: news-please
        result = self._extract_with_newsplease(url, content)
        if result:
            return result

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: trafilatura (fundus —Ç—Ä–µ–±—É–µ—Ç —Å–ª–æ–∂–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏)
        result = self._extract_with_trafilatura(url, content)
        if result:
            return result

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: AutoScraper
        result = self._extract_with_autoscraper(url, content)
        if result:
            return result

        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ {url}")
        return None

    async def _process_source(self, category: str, subcategory: str, name: str, url: str) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.

        Args:
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–æ–≤–æ—Å—Ç–∏
            subcategory: –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–æ–≤–æ—Å—Ç–∏
            name: –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            url: URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        start_time = time.time()

        async with self.semaphore:
            try:
                logger.info(f"[{category}/{subcategory}] {url} -> START")

                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º
                success, content_type, content = await self._fetch_content_with_retry(url, max_retries=3)
                if not success or not content:
                    logger.warning(f"[{category}/{subcategory}] {url} -> FAIL (fetch)")
                    return {"success": False, "reason": "fetch_failed"}

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                if "rss" in content_type or "xml" in content_type or url.endswith((".rss", ".xml")):
                    # RSS –∏—Å—Ç–æ—á–Ω–∏–∫
                    return await self._process_rss_source(category, subcategory, name, url, content)
                else:
                    # HTML –∏—Å—Ç–æ—á–Ω–∏–∫
                    return await self._process_html_source(category, subcategory, name, url, content)

            except Exception as e:
                logger.error(f"[{category}/{subcategory}] {url} -> ERROR: {e}")
                return {"success": False, "reason": str(e)}
            finally:
                elapsed = time.time() - start_time
                logger.debug(f"[{category}/{subcategory}] {url} -> completed in {elapsed:.2f}s")

    async def _process_rss_source(
        self, category: str, subcategory: str, name: str, url: str, content: bytes
    ) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∞."""
        try:
            feed = feedparser.parse(content)

            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS
            if feed.bozo:
                logger.warning(f"RSS parsing error for {url}: {feed.bozo_exception}")
                # –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∏—Å–ø—Ä–∞–≤–∏—Ç—å encoding
                if "encoding" in str(feed.bozo_exception).lower():
                    try:
                        content_str = content.decode("utf-8", errors="ignore")
                        feed = feedparser.parse(content_str)
                    except Exception as e:
                        logger.debug(f"Failed to fix encoding: {e}")

            if not feed.entries:
                return {"success": False, "reason": "no_entries"}

            processed_count = 0
            saved_count = 0

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ 50 –∑–∞–ø–∏—Å–µ–π –≤–º–µ—Å—Ç–æ 5
            max_entries = getattr(self, "max_rss_entries", 50)
            for entry in feed.entries[:max_entries]:
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ RSS
                    title = clean_text(entry.get("title", ""))
                    link = entry.get("link", "")

                    # –ö–∞—Å–∫–∞–¥–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: content > description > summary
                    article_content = ""
                    if hasattr(entry, "content") and entry.content:
                        article_content = clean_text(entry.content[0].value)
                    elif hasattr(entry, "description") and entry.description:
                        article_content = clean_text(entry.description)
                    else:
                        article_content = clean_text(entry.get("summary", ""))

                    # –ò–∑–≤–ª–µ—á—å –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                    from datetime import datetime, timezone

                    pub_date = None
                    if hasattr(entry, "published_parsed") and entry.published_parsed:
                        try:
                            pub_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                        except Exception:
                            pass
                    elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
                        try:
                            pub_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                        except Exception:
                            pass

                    if not title:
                        continue

                    processed_count += 1

                    # –û—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
                    text_for_ai = f"{title} {article_content}".strip()
                    if not text_for_ai:
                        continue

                    importance = evaluate_importance({"title": title, "content": text_for_ai})
                    credibility = evaluate_credibility({"title": title, "content": text_for_ai})

                    if importance < self.min_importance:
                        logger.debug(f"[{category}/{subcategory}] {title} -> SKIP (importance: {importance:.2f})")
                        continue

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
                    news_item = {
                        "title": title,
                        "content": article_content,
                        "link": link,
                        "source": name,
                        "category": category,
                        "subcategory": subcategory,
                        "importance": importance,
                        "credibility": credibility,
                        "published_at": pub_date,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∞–ª—å–Ω—É—é –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                    }

                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –ë–î
                    db_service = get_async_service()
                    await db_service.async_upsert_news([news_item])
                    saved_count += 1

                    logger.debug(f"[{category}/{subcategory}] {title} -> SAVED (importance: {importance:.2f})")

                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ RSS –∑–∞–ø–∏—Å–∏: {e}")
                    continue

            return {
                "success": True,
                "processed": processed_count,
                "saved": saved_count,
                "type": "rss",
            }

        except Exception as e:
            return {"success": False, "reason": f"rss_parse_error: {e}"}

    async def _process_html_source(
        self, category: str, subcategory: str, name: str, url: str, content: bytes
    ) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ HTML –∏—Å—Ç–æ—á–Ω–∏–∫–∞."""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∫–∞—Å–∫–∞–¥–Ω—ã–º –º–µ—Ç–æ–¥–æ–º
            extracted = self._extract_content_cascade(url, content)
            if not extracted:
                return {"success": False, "reason": "content_extraction_failed"}

            title = extracted["title"]
            maintext = extracted["maintext"]
            method = extracted["method"]

            if not title or not maintext:
                return {"success": False, "reason": "insufficient_content"}

            # –û—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
            text_for_ai = f"{title} {maintext}".strip()
            importance = evaluate_importance({"title": title, "content": text_for_ai})
            credibility = evaluate_credibility({"title": title, "content": text_for_ai})

            if importance < self.min_importance:
                logger.debug(f"[{category}/{subcategory}] {title} -> SKIP (importance: {importance:.2f})")
                return {"success": False, "reason": "low_importance", "importance": importance}

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            news_item = {
                "title": title,
                "content": maintext,
                "link": url,
                "source": name,
                "category": category,
                "subcategory": subcategory,
                "importance": importance,
                "credibility": credibility,
            }

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –ë–î
            db_service = get_async_service()
            await db_service.async_upsert_news([news_item])

            logger.info(f"[{category}/{subcategory}] {url} -> SUCCESS ({method}, importance: {importance:.2f})")

            return {
                "success": True,
                "processed": 1,
                "saved": 1,
                "type": "html",
                "method": method,
                "importance": importance,
                "credibility": credibility,
            }

        except Exception as e:
            return {"success": False, "reason": f"html_parse_error: {e}"}

    async def run(self, auto_retrain: bool = True) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

        Args:
            auto_retrain: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        if not self.session:
            await self._init_session()

        sources = self._get_all_sources()
        if not sources:
            logger.warning("–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return {"error": "no_sources"}

        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")

        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        tasks = []
        for category, subcategory, name, url in sources:
            task = self._process_source(category, subcategory, name, url)
            tasks.append(task)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        stats = {
            "total_sources": len(sources),
            "successful": 0,
            "failed": 0,
            "total_processed": 0,
            "total_saved": 0,
            "errors": [],
        }

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                stats["failed"] += 1
                stats["errors"].append(f"Source {i}: {result}")
                continue

            if result.get("success"):
                stats["successful"] += 1
                stats["total_processed"] += result.get("processed", 0)
                stats["total_saved"] += result.get("saved", 0)
            else:
                stats["failed"] += 1

        logger.info(
            f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {stats['successful']}/{stats['total_sources']} —É—Å–ø–µ—à–Ω–æ, "
            f"{stats['total_saved']} –Ω–æ–≤–æ—Å—Ç–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ"
        )

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞
        if auto_retrain and stats.get("total_saved", 0) > 0:
            try:
                logger.info("")
                logger.info("ü§ñ –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...")

                from ai_modules.self_tuning_collector import get_self_tuning_collector
                from ai_modules.self_tuning_trainer import get_self_tuning_trainer
                from datetime import datetime, timezone, timedelta

                collector = get_self_tuning_collector()
                trainer = get_self_tuning_trainer()

                # –ü—Ä–æ–≤–µ—Ä–∫–∞, –≤–∫–ª—é—á–µ–Ω–æ –ª–∏ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ
                if not collector.is_enabled() or not trainer.is_enabled():
                    logger.info("‚è≠Ô∏è Self-tuning –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
                else:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
                    metadata = trainer._load_existing_metadata()
                    should_train = False

                    if not metadata:
                        should_train = True
                        logger.info("üìä –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
                    else:
                        last_training_str = metadata.get("timestamp")
                        if last_training_str:
                            last_training = datetime.fromisoformat(last_training_str.replace("Z", "+00:00"))
                            interval_days = trainer.config.get("self_tuning", {}).get("interval_days", 2)
                            next_training = last_training + timedelta(days=interval_days)
                            now = datetime.now(timezone.utc)

                            if now >= next_training:
                                should_train = True
                                days_since = (now - last_training).days
                                logger.info(f"‚úÖ –ò–Ω—Ç–µ—Ä–≤–∞–ª –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –¥–æ—Å—Ç–∏–≥–Ω—É—Ç ({days_since} –¥–Ω–µ–π)")
                            else:
                                remaining = next_training - now
                                logger.info(f"‚è≥ –î–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {remaining}")
                        else:
                            should_train = True

                    if should_train:
                        # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
                        logger.info("üìä –°–±–æ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
                        collection_result = collector.collect_training_data()

                        if collection_result["success"]:
                            dataset_size = collection_result["dataset_size"]
                            logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {dataset_size} –ø—Ä–∏–º–µ—Ä–æ–≤")

                            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
                            logger.info("üß† –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
                            from pathlib import Path

                            dataset_path = Path(collection_result["dataset_path"])
                            training_result = trainer.train_models(dataset_path)

                            if training_result["success"]:
                                improvements = training_result.get("improvements", {})
                                for model_name, result in improvements.items():
                                    f1_score = result.get("f1_score", 0.0)
                                    replaced = result.get("replaced", False)
                                    status = "‚úÖ –ó–ê–ú–ï–ù–ï–ù–ê" if replaced else "‚è∏Ô∏è –ù–ï –ó–ê–ú–ï–ù–ï–ù–ê"
                                    logger.info(f"   {model_name}: F1={f1_score:.3f} ({status})")

                                logger.info("üéâ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
                                stats["retrain_status"] = "success"
                                stats["retrain_improvements"] = improvements
                            else:
                                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {training_result.get('error')}")
                                stats["retrain_status"] = "training_failed"
                        else:
                            logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {collection_result.get('error')}")
                            stats["retrain_status"] = "insufficient_data"
                    else:
                        logger.info("‚è≠Ô∏è –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è (–∏–Ω—Ç–µ—Ä–≤–∞–ª –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç)")
                        stats["retrain_status"] = "skipped_interval"

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {e}")
                stats["retrain_status"] = "error"
                stats["retrain_error"] = str(e)

        return stats


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def main():
    """–ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞."""
    async with AdvancedParser(max_concurrent=10, min_importance=0.3) as parser:
        stats = await parser.run()
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞: {stats}")


if __name__ == "__main__":
    asyncio.run(main())
