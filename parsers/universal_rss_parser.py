"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π RSS –ø–∞—Ä—Å–µ—Ä —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤.
"""

import hashlib
import logging
import re
import time
from datetime import datetime, timezone
from typing import Dict, List, Optional, Tuple, Any
from urllib.parse import urljoin, urlparse

import requests
import feedparser
from dateutil import parser as dtp
from bs4 import BeautifulSoup

from utils.clean_text import clean_text
from services.categories import get_all_sources

logger = logging.getLogger("parsers.universal_rss")

# –£–ª—É—á—à–µ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –ª—É—á—à–µ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "application/rss+xml, application/xml, text/xml, application/atom+xml, text/html, */*",
    "Accept-Language": "en-US,en;q=0.9",
    "Accept-Encoding": "gzip, deflate, br",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
}

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è retry –ª–æ–≥–∏–∫–∏
MAX_RETRIES = 3
RETRY_DELAY = 2  # —Å–µ–∫—É–Ω–¥—ã
TIMEOUT = 15  # —Å–µ–∫—É–Ω–¥—ã


class UniversalRSSParser:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π RSS –ø–∞—Ä—Å–µ—Ä —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

    def parse_source(
        self, url: str, category: str, subcategory: str, source_name: str
    ) -> List[Dict]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ RSS.

        Args:
            url: URL RSS —Ñ–∏–¥–∞
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–æ–≤–æ—Å—Ç–∏
            subcategory: –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–æ–≤–æ—Å—Ç–∏
            source_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞

        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–ª—è–º–∏ category –∏ subcategory
        """
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å retry –ª–æ–≥–∏–∫–æ–π
            feed = self._fetch_feed_with_retry(url)
            if not feed or not feed.entries:
                logger.warning(f"–ü—É—Å—Ç–æ–π —Ñ–∏–¥: {source_name} ({url})")
                return []

            news_items = []
            for entry in feed.entries:
                try:
                    news_item = self._parse_entry(entry, category, subcategory, source_name, url)
                    if news_item:
                        news_items.append(news_item)
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø–∏—Å–∏ –∏–∑ {source_name}: {e}")
                    continue

            logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ {source_name}: {len(news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π")
            return news_items

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source_name}: {e}")
            return []

    def _fetch_feed_with_retry(self, url: str) -> Optional[Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∏–¥ —Å retry –ª–æ–≥–∏–∫–æ–π –∏ —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π Content-Type."""
        for attempt in range(MAX_RETRIES):
            try:
                logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{MAX_RETRIES} –∑–∞–≥—Ä—É–∑–∫–∏: {url}")

                response = self.session.get(url, timeout=TIMEOUT, allow_redirects=True)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–æ–¥
                if response.status_code != 200:
                    logger.warning(f"HTTP {response.status_code} –¥–ª—è {url}")
                    if attempt < MAX_RETRIES - 1:
                        time.sleep(RETRY_DELAY * (attempt + 1))
                        continue
                    return None

                # –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ Content-Type
                content_type = response.headers.get("Content-Type", "").lower()
                content = response.content

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç XML/RSS
                if self._is_rss_content(content, content_type, url):
                    feed = feedparser.parse(content)
                    if feed.bozo and not feed.entries:
                        logger.warning(f"–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å —Ñ–∏–¥–æ–º {url}: {feed.bozo_exception}")
                    return feed
                else:
                    logger.warning(f"–ù–µ RSS –∫–æ–Ω—Ç–µ–Ω—Ç: {url} (Content-Type: {content_type})")
                    if attempt < MAX_RETRIES - 1:
                        time.sleep(RETRY_DELAY)
                        continue
                    return None

            except requests.exceptions.RequestException as e:
                logger.warning(f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –¥–ª—è {url} (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY * (attempt + 1))
                    continue
                return None
            except Exception as e:
                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è {url}: {e}")
                return None

        return None

    def _is_rss_content(self, content: bytes, content_type: str, url: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç RSS/XML —Ñ–∏–¥–æ–º."""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Content-Type
        if any(xml_type in content_type for xml_type in ['xml', 'rss', 'atom']):
            return True

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_start = content[:500].decode('utf-8', errors='ignore').lower()

        # –ò—â–µ–º RSS/XML —Ç–µ–≥–∏
        if any(tag in content_start for tag in ['<rss', '<feed', '<rdf:rdf', '<?xml']):
            return True

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if any(pattern in url.lower() for pattern in ['.rss', '.xml', 'rss', 'feed']):
            return True

        return False

    def _parse_entry(
        self, entry: Any, category: str, subcategory: str, source_name: str, base_url: str
    ) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É –∑–∞–ø–∏—Å—å –∏–∑ RSS —Ñ–∏–¥–∞."""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ –æ—á–∏—â–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = self._extract_title(entry)
            if not title:
                return None

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ –æ—á–∏—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            content = self._extract_content(entry)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É
            link = self._extract_link(entry, base_url)

            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
            published_at = self._extract_date(entry)

            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
            uid = hashlib.md5(f"{link}{title}".encode()).hexdigest()

            return {
                "uid": uid,
                "title": title,
                "content": content,
                "link": link,
                "source": source_name,
                "category": category,
                "subcategory": subcategory,
                "published_at": published_at.isoformat() if published_at else None,
            }

        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∑–∞–ø–∏—Å–∏: {e}")
            return None

    def _extract_title(self, entry: Any) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ –∑–∞–ø–∏—Å–∏."""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–ª—è –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_fields = ['title', 'title_detail', 'summary']

        for field in title_fields:
            if hasattr(entry, field):
                title = entry.get(field, '')
                if isinstance(title, dict):
                    title = title.get('value', '')
                if title:
                    return clean_text(str(title))

        return ""

    def _extract_content(self, entry: Any) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –∑–∞–ø–∏—Å–∏."""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–ª—è –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_fields = ['summary', 'content', 'description']

        for field in content_fields:
            if hasattr(entry, field):
                content = entry.get(field, '')
                if isinstance(content, dict):
                    content = content.get('value', '')
                if isinstance(content, list) and content:
                    content = (
                        content[0].get('value', '')
                        if isinstance(content[0], dict)
                        else str(content[0])
                    )

                if content:
                    # –û—á–∏—â–∞–µ–º HTML —Ç–µ–≥–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    cleaned_content = clean_text(str(content))
                    if cleaned_content and len(cleaned_content) > 10:
                        return cleaned_content

        return ""

    def _extract_link(self, entry: Any, base_url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫—É –∏–∑ –∑–∞–ø–∏—Å–∏."""
        link = entry.get('link', '')
        if link:
            # –î–µ–ª–∞–µ–º —Å—Å—ã–ª–∫—É –∞–±—Å–æ–ª—é—Ç–Ω–æ–π
            return urljoin(base_url, link)
        return ""

    def _extract_date(self, entry: Any) -> Optional[datetime]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –∏–∑ –∑–∞–ø–∏—Å–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º."""
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–ª—è –¥–ª—è –¥–∞—Ç—ã
        date_fields = ['published', 'updated', 'created', 'pubDate']

        for field in date_fields:
            if hasattr(entry, field):
                date_str = entry.get(field, '')
                if date_str:
                    parsed_date = self._parse_date(date_str)
                    if parsed_date:
                        return parsed_date

        # –ü—Ä–æ–±—É–µ–º parsed –¥–∞—Ç—ã
        if hasattr(entry, 'published_parsed') and entry.published_parsed:
            try:
                return datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
            except:
                pass

        if hasattr(entry, 'updated_parsed') and entry.updated_parsed:
            try:
                return datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
            except:
                pass

        return None

    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """–ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π."""
        if not date_str:
            return None

        try:
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
            date_str = re.sub(r'[^\w\s:+-]', '', str(date_str))

            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É
            dt = dtp.parse(date_str)

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º timezone –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
            if not dt.tzinfo:
                dt = dt.replace(tzinfo=timezone.utc)

            return dt.astimezone(timezone.utc)

        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É '{date_str}': {e}")
            return None

    def fetch_all_sources(self, per_source_limit: Optional[int] = None) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
        all_sources = get_all_sources()
        news_items = []
        seen = set()

        logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {len(all_sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")

        successful_sources = 0
        failed_sources = 0

        for cat, subcat, name, url in all_sources:
            logger.info(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {name} ({url})")

            source_items = self.parse_source(url, cat, subcat, name)

            if source_items:
                successful_sources += 1

                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–∏–º–∏—Ç –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
                if per_source_limit:
                    source_items = source_items[:per_source_limit]

                # –î–æ–±–∞–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                for item in source_items:
                    uid = item["uid"]
                    if uid not in seen:
                        seen.add(uid)
                        news_items.append(item)

                logger.info(f"‚úÖ {name}: {len(source_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–æ–±–∞–≤–ª–µ–Ω–æ")
            else:
                failed_sources += 1
                logger.warning(f"‚ùå {name}: –Ω–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π")

        logger.info(
            f"üìä –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {successful_sources} —É—Å–ø–µ—à–Ω—ã—Ö, {failed_sources} –Ω–µ—É–¥–∞—á–Ω—ã—Ö, {len(news_items)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"
        )
        return news_items

    def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–µ—Å—Å–∏—é."""
        if hasattr(self, 'session'):
            self.session.close()


# Convenience functions –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def parse_source(url: str, category: str, subcategory: str, source_name: str) -> List[Dict]:
    """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—Ç–∞—Ä—ã–º API."""
    parser = UniversalRSSParser()
    try:
        return parser.parse_source(url, category, subcategory, source_name)
    finally:
        parser.close()


def fetch_rss(urls: Dict[str, Dict], per_source_limit: Optional[int] = None) -> List[Dict]:
    """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—Ç–∞—Ä—ã–º API."""
    parser = UniversalRSSParser()
    try:
        all_sources = []
        for meta in urls.values():
            source_items = parser.parse_source(
                url=meta["url"],
                category=meta.get("category", ""),
                subcategory=meta.get("subcategory", ""),
                source_name=meta["name"],
            )

            if per_source_limit:
                source_items = source_items[:per_source_limit]

            all_sources.extend(source_items)

        return all_sources
    finally:
        parser.close()
