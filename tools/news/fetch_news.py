#!/usr/bin/env python3
"""
–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π.
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å fetch_and_store_news.py, fetch_loop.py, fetch_optimized.py
"""


# === –ò–ó fetch_and_store_news.py ===

#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç AdvancedParser –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤,
–ø—Ä–∏–º–µ–Ω—è–µ—Ç AI-—Ñ–∏–ª—å—Ç—Ä—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.

–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    python tools/fetch_and_store_news.py
    python tools/fetch_and_store_news.py --min-importance 0.5 --max-concurrent 5
"""

from parsers.advanced_parser import AdvancedParser
import asyncio
import argparse
import logging
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent))


# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("logs/advanced_parser.log", encoding="utf-8"),
    ],
)

logger = logging.getLogger(__name__)


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞."""
    parser = argparse.ArgumentParser(description="–ó–∞–ø—É—Å–∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π")
    parser.add_argument(
        "--min-importance",
        type=float,
        default=0.3,
        help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.3)",
    )
    parser.add_argument(
        "--max-concurrent",
        type=int,
        default=10,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)",
    )
    parser.add_argument("--verbose", action="store_true", help="–ü–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ")

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π")
    logger.info(
        f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: min_importance={args.min_importance}, max_concurrent={args.max_concurrent}")

    try:
        async with AdvancedParser(
            max_concurrent=args.max_concurrent, min_importance=args.min_importance
        ) as parser_instance:

            stats = await parser_instance.run()

            logger.info("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω")
            logger.info(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}")

            # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            print("\n" + "=" * 60)
            print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–ê–†–°–ò–ù–ì–ê")
            print("=" * 60)
            print(f"üì∞ –í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {stats.get('total_sources', 0)}")
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats.get('successful', 0)}")
            print(f"‚ùå –ù–µ—É–¥–∞—á–Ω–æ: {stats.get('failed', 0)}")
            print(f"üîÑ –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats.get('total_processed', 0)}")
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î: {stats.get('total_saved', 0)}")

            if stats.get("errors"):
                print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∏ ({len(stats['errors'])}):")
                for error in stats["errors"][:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –æ—à–∏–±–æ–∫
                    print(f"   ‚Ä¢ {error}")
                if len(stats["errors"]) > 5:
                    print(f"   ... –∏ –µ—â–µ {len(stats['errors']) - 5} –æ—à–∏–±–æ–∫")

            print("=" * 60)

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–¥ –≤—ã—Ö–æ–¥–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if stats.get("total_saved", 0) > 0:
                return 0  # –£—Å–ø–µ—Ö
            else:
                return 1  # –ù–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π

    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        return 2


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)


# === –ò–ó fetch_loop.py ===

#!/usr/bin/env python3
"""
Fetch Loop with Auto-Posting Integration.

This script runs a continuous loop to fetch news, generate digests,
and automatically post them to Telegram channels.
"""

from ai_modules.metrics import get_metrics
from telegram_bot.handlers.digest_handler import get_digest_handler, auto_post_digest
from parsers.optimized_parser import run_optimized_parser
import asyncio
import argparse
import logging
import signal
import sys
from datetime import datetime, timezone
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))


# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("logs/fetch_loop.log", encoding="utf-8"),
    ],
)

logger = logging.getLogger(__name__)


class FetchLoop:
    """
    Continuous fetch loop with auto-posting integration.

    Features:
    - Configurable intervals
    - AI filtering
    - Auto-posting to Telegram
    - Graceful shutdown
    - Metrics tracking
    """

    def __init__(self, interval: int = 30, ai_filter: bool = True, auto_post: bool = False):
        """Initialize fetch loop."""
        self.interval = interval
        self.ai_filter = ai_filter
        self.auto_post = auto_post
        self.running = False
        self.metrics = get_metrics()

        # Setup signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

        logger.info(
            f"FetchLoop initialized: interval={interval}s, ai_filter={ai_filter}, auto_post={auto_post}")

    def _signal_handler(self, signum, frame):
        """Handle shutdown signals."""
        logger.info(f"Received signal {signum}, shutting down gracefully...")
        self.running = False

    async def _run_fetch_cycle(self) -> bool:
        """
        Run a single fetch cycle.

        Returns:
            True if successful, False otherwise
        """
        try:
            logger.info("Starting fetch cycle...")
            start_time = datetime.now(timezone.utc)

            # Run optimized parser
            if self.ai_filter:
                logger.info("Running with AI filtering enabled")
                result = await run_optimized_parser()
            else:
                logger.info("Running without AI filtering")
                result = await run_optimized_parser()

            if not result.get("success", False):
                logger.error(f"Fetch cycle failed: {result.get('error', 'Unknown error')}")
                return False

            # Log results
            processed = result.get("processed", 0)
            saved = result.get("saved", 0)
            ai_calls = result.get("ai_calls", 0)

            logger.info(
                f"Fetch cycle completed: processed={processed}, saved={saved}, ai_calls={ai_calls}")

            # Run auto-posting if enabled
            if self.auto_post:
                logger.info("Running auto-posting...")
                post_result = await auto_post_digest()

                if post_result.get("success", False):
                    published = post_result.get("published_count", 0)
                    logger.info(f"Auto-posting completed: published={published} digests")
                else:
                    logger.warning(
                        f"Auto-posting failed: {post_result.get('reason', 'Unknown error')}")

            # Calculate cycle time
            cycle_time = (datetime.now(timezone.utc) - start_time).total_seconds()
            logger.info(f"Cycle completed in {cycle_time:.2f} seconds")

            return True

        except Exception as e:
            logger.error(f"Error in fetch cycle: {e}")
            return False

    async def run(self):
        """Run the continuous fetch loop."""
        self.running = True
        logger.info("Starting fetch loop...")

        cycle_count = 0

        while self.running:
            try:
                cycle_count += 1
                logger.info(f"Starting cycle #{cycle_count}")

                # Run fetch cycle
                success = await self._run_fetch_cycle()

                if success:
                    logger.info(f"Cycle #{cycle_count} completed successfully")
                else:
                    logger.error(f"Cycle #{cycle_count} failed")

                # Wait for next cycle
                if self.running:
                    logger.info(f"Waiting {self.interval} seconds until next cycle...")
                    await asyncio.sleep(self.interval)

            except asyncio.CancelledError:
                logger.info("Fetch loop cancelled")
                break
            except Exception as e:
                logger.error(f"Unexpected error in fetch loop: {e}")
                if self.running:
                    logger.info(f"Waiting {self.interval} seconds before retry...")
                    await asyncio.sleep(self.interval)

        logger.info("Fetch loop stopped")

    async def stop(self):
        """Stop the fetch loop gracefully."""
        logger.info("Stopping fetch loop...")
        self.running = False


async def main():
    """Main function."""
    parser = argparse.ArgumentParser(description="Fetch loop with auto-posting")
    parser.add_argument(
        "--interval",
        type=int,
        default=30,
        help="Interval between cycles in seconds (default: 30)")
    parser.add_argument("--ai-filter", action="store_true", help="Enable AI filtering")
    parser.add_argument("--auto-post", action="store_true", help="Enable auto-posting to Telegram")
    parser.add_argument(
        "--once",
        action="store_true",
        help="Run only once instead of continuous loop")

    args = parser.parse_args()

    # Create fetch loop
    fetch_loop = FetchLoop(
        interval=args.interval,
        ai_filter=args.ai_filter,
        auto_post=args.auto_post)

    try:
        if args.once:
            # Run single cycle
            logger.info("Running single fetch cycle...")
            success = await fetch_loop._run_fetch_cycle()
            if success:
                logger.info("Single cycle completed successfully")
                sys.exit(0)
            else:
                logger.error("Single cycle failed")
                sys.exit(1)
        else:
            # Run continuous loop
            await fetch_loop.run()

    except KeyboardInterrupt:
        logger.info("Received keyboard interrupt, shutting down...")
        await fetch_loop.stop()
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Shutdown complete")
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)


# === –ò–ó fetch_optimized.py ===

#!/usr/bin/env python3
"""
Optimized news fetching tool with AI call reduction.

This script uses the optimized parser to fetch news with reduced AI API calls
while maintaining quality through pre-filtering, caching, and local prediction.

Example usage:
    python tools/fetch_optimized.py
    python tools/fetch_optimized.py --max-concurrent 5 --min-importance 0.6
    python tools/fetch_optimized.py --enable-local-predictor --disable-cache
"""

from ai_modules.metrics import get_metrics
from parsers.optimized_parser import run_optimized_parser
import asyncio
import argparse
import logging
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent))


# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("logs/fetch_optimized.log", encoding="utf-8"),
    ],
)

logger = logging.getLogger(__name__)


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞."""
    parser = argparse.ArgumentParser(description="–ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π")
    parser.add_argument(
        "--max-concurrent",
        type=int,
        default=10,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)",
    )
    parser.add_argument(
        "--min-importance",
        type=float,
        default=0.6,
        help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –≤–∞–∂–Ω–æ—Å—Ç–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.6)",
    )
    parser.add_argument(
        "--min-credibility",
        type=float,
        default=0.7,
        help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.7)",
    )
    parser.add_argument(
        "--enable-local-predictor",
        action="store_true",
        help="–í–∫–ª—é—á–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä",
    )
    parser.add_argument(
        "--disable-cache",
        action="store_true",
        help="–û—Ç–∫–ª—é—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ",
    )
    parser.add_argument(
        "--disable-prefilter",
        action="store_true",
        help="–û—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–µ–¥—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é",
    )
    parser.add_argument(
        "--show-metrics",
        action="store_true",
        help="–ü–æ–∫–∞–∑–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è",
    )
    parser.add_argument(
        "--reset-metrics",
        action="store_true",
        help="–°–±—Ä–æ—Å–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º",
    )

    args = parser.parse_args()

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    import os

    if args.enable_local_predictor:
        os.environ["LOCAL_PREDICTOR_ENABLED"] = "true"

    if args.disable_cache:
        os.environ["CACHE_ENABLED"] = "false"

    if args.disable_prefilter:
        os.environ["PREFILTER_ENABLED"] = "false"

    # –°–±—Ä–æ—Å –º–µ—Ç—Ä–∏–∫ –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω
    if args.reset_metrics:
        metrics = get_metrics()
        metrics.reset_metrics()
        logger.info("–ú–µ—Ç—Ä–∏–∫–∏ —Å–±—Ä–æ—à–µ–Ω—ã")

    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π")
    logger.info(
        f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: concurrent={args.max_concurrent}, "
        f"importance‚â•{args.min_importance}, credibility‚â•{args.min_credibility}"
    )

    try:
        # –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
        result = await run_optimized_parser(max_concurrent=args.max_concurrent)

        if result["success"]:
            logger.info("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            logger.info(f"üì∞ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {result['processed_items']}")
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î: {result['saved_items']}")
            logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {result['processing_time_seconds']}s")

            # –ü–æ–∫–∞–∑–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            metrics_summary = result.get("metrics", {})
            logger.info("üìà –ú–µ—Ç—Ä–∏–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:")
            logger.info(f"   ü§ñ AI –≤—ã–∑–æ–≤–æ–≤: {metrics_summary.get('ai_calls_total', 0)}")
            logger.info(
                f"   üö´ –ü—Ä–æ–ø—É—â–µ–Ω–æ –ø—Ä–µ–¥—Ñ–∏–ª—å—Ç—Ä–æ–º: {metrics_summary.get('ai_skipped_prefilter_total', 0)}")
            logger.info(f"   üíæ –ü—Ä–æ–ø—É—â–µ–Ω–æ –∫—ç—à–µ–º: {metrics_summary.get('ai_skipped_cache_total', 0)}")
            logger.info(
                f"   üß† –ü—Ä–æ–ø—É—â–µ–Ω–æ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–æ–º: {metrics_summary.get('ai_skipped_local_pred_total', 0)}")
            logger.info(
                f"   üí∞ –í—Å–µ–≥–æ —Å—ç–∫–æ–Ω–æ–º–ª–µ–Ω–æ –≤—ã–∑–æ–≤–æ–≤: {metrics_summary.get('ai_calls_saved_total', 0)}")
            logger.info(f"   üìä –≠–∫–æ–Ω–æ–º–∏—è: {metrics_summary.get('ai_calls_saved_percentage', 0)}%")

            if args.show_metrics:
                print("\n" + "=" * 60)
                print("üìä –ü–û–î–†–û–ë–ù–´–ï –ú–ï–¢–†–ò–ö–ò")
                print("=" * 60)
                for key, value in metrics_summary.items():
                    print(f"{key}: {value}")
                print("=" * 60)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            ai_calls_saved = metrics_summary.get("ai_calls_saved_total", 0)
            news_processed = metrics_summary.get("news_processed_total", 0)

            if news_processed > 0:
                efficiency = (ai_calls_saved / news_processed) * 100
                logger.info(f"üéØ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {efficiency:.1f}%")

                if efficiency >= 60:
                    logger.info("üéâ –û—Ç–ª–∏—á–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å! –¶–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ (‚â•60%)")
                elif efficiency >= 30:
                    logger.info("‚úÖ –•–æ—Ä–æ—à–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å")
                else:
                    logger.warning("‚ö†Ô∏è –ù–∏–∑–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫—É –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")

        else:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
            sys.exit(1)

    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
