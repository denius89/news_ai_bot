#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ PulseAI.

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö RSS-—Ñ–∏–¥–æ–≤ —Å –ø–æ–º–æ—â—å—é AdvancedParser,
—É–¥–∞–ª—è–µ—Ç –Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏–µ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –∏–∑ GitHub-—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤.
"""

import asyncio
import logging
import yaml
import aiohttp
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
from urllib.parse import urljoin, urlparse
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent))

from parsers.advanced_parser import AdvancedParser
from database.service import get_async_service

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/rss_update.log', encoding='utf-8')
    ]
)

logger = logging.getLogger(__name__)


class RSSUpdater:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RSS Updater."""
        self.config_path = Path("config/sources.yaml")
        self.backup_path = Path(f"config/sources.backup.{datetime.now().strftime('%Y%m%d')}.yaml")
        self.sources_config = {}
        self.parser = None
        self.session = None
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'checked': 0,
            'removed': 0,
            'added': 0,
            'updated_categories': set()
        }
        
        # GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ —Å RSS-—Ñ–∏–¥–∞–º–∏
        self.github_sources = [
            "https://raw.githubusercontent.com/plenaryapp/awesome-rss-feeds/main/README.md",
            "https://raw.githubusercontent.com/mclassy/Cryptocurrency-RSS-Feed-List/main/README.md",
            "https://raw.githubusercontent.com/chainfeeds/RSSAggregatorforWeb3/main/README.md",
            "https://raw.githubusercontent.com/tuan3w/awesome-tech-rss/main/README.md",
            "https://raw.githubusercontent.com/foorilla/allinfosecnews_sources/main/README.md",
            "https://raw.githubusercontent.com/voidfiles/awesome-rss/main/README.md",
            "https://raw.githubusercontent.com/joshuawalcher/rssfeeds/main/README.md",
            "https://raw.githubusercontent.com/mcnaveen/awesome-rss/main/README.md"
        ]
        
        # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
        self.category_keywords = {
            'crypto': {
                'bitcoin': ['bitcoin', 'btc', 'bitcoinmagazine'],
                'ethereum': ['ethereum', 'eth', 'ethereum.org'],
                'altcoins': ['altcoin', 'cryptocurrency', 'coin', 'crypto'],
                'defi': ['defi', 'decentralized', 'uniswap', 'compound'],
                'nft': ['nft', 'non-fungible', 'opensea'],
                'gamefi': ['gamefi', 'gaming', 'play-to-earn'],
                'regulation': ['regulation', 'sec', 'cftc', 'legal'],
                'exchanges': ['exchange', 'binance', 'coinbase'],
                'security': ['security', 'hack', 'exploit', 'vulnerability']
            },
            'markets': {
                'stocks': ['stock', 'equity', 'nasdaq', 'nyse'],
                'commodities': ['commodity', 'gold', 'oil', 'silver'],
                'forex': ['forex', 'currency', 'fx', 'dollar'],
                'bonds': ['bond', 'treasury', 'yield'],
                'central_banks': ['fed', 'federal reserve', 'ecb', 'central bank'],
                'economic_data': ['economic', 'gdp', 'inflation', 'unemployment']
            },
            'tech': {
                'ai': ['ai', 'artificial intelligence', 'machine learning', 'openai'],
                'bigtech': ['google', 'apple', 'microsoft', 'amazon', 'meta'],
                'startups': ['startup', 'venture', 'funding', 'unicorn'],
                'cybersecurity': ['security', 'cyber', 'hack', 'breach'],
                'hardware': ['hardware', 'cpu', 'gpu', 'chip'],
                'blockchain_tech': ['blockchain', 'smart contract', 'web3']
            },
            'sports': {
                'football': ['football', 'soccer', 'premier league', 'champions league'],
                'basketball': ['basketball', 'nba', 'ncaa'],
                'tennis': ['tennis', 'wimbledon', 'us open'],
                'esports': ['esports', 'gaming', 'twitch', 'streaming']
            },
            'world': {
                'elections': ['election', 'vote', 'presidential', 'parliament'],
                'geopolitics': ['geopolitics', 'diplomacy', 'international'],
                'conflicts': ['conflict', 'war', 'military', 'defense'],
                'energy': ['energy', 'oil', 'gas', 'renewable'],
                'climate': ['climate', 'environment', 'carbon', 'green'],
                'diplomacy': ['diplomacy', 'un', 'nato', 'summit']
            }
        }
        
    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥."""
        await self._init_session()
        await self._load_config()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥."""
        await self._close_session()
        
    async def _init_session(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è HTTP —Å–µ—Å—Å–∏–∏."""
        timeout = aiohttp.ClientTimeout(total=10, connect=5)
        connector = aiohttp.TCPConnector(limit=100, limit_per_host=5)
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            headers=headers
        )
        
    async def _close_session(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ HTTP —Å–µ—Å—Å–∏–∏."""
        if self.session:
            await self.session.close()
            
    async def _load_config(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
        if not self.config_path.exists():
            logger.error(f"–§–∞–π–ª {self.config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
            
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.sources_config = yaml.safe_load(f) or {}
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ {self.config_path}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            self.sources_config = {}
            
    async def _save_config(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
        try:
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
            if self.config_path.exists():
                import shutil
                shutil.copy2(self.config_path, self.backup_path)
                logger.info(f"–°–æ–∑–¥–∞–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è: {self.backup_path}")
                
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.dump(self.sources_config, f, default_flow_style=False, sort_keys=True)
            logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {self.config_path}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            
    async def _validate_rss_feed(self, url: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ RSS-—Ñ–∏–¥–∞.
        
        Args:
            url: URL RSS-—Ñ–∏–¥–∞
            
        Returns:
            True –µ—Å–ª–∏ —Ñ–∏–¥ –≤–∞–ª–∏–¥–µ–Ω, False –∏–Ω–∞—á–µ
        """
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ HTTP –∑–∞–ø—Ä–æ—Å–∞
            async with self.session.get(url) as response:
                if response.status != 200:
                    return False
                    
                content = await response.text()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ XML/RSS
                content_lower = content.lower()
                
                # –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å RSS –∏–ª–∏ Atom —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                if not ('<rss' in content_lower or '<feed' in content_lower):
                    return False
                    
                # –î–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–æ–≤–æ—Å—Ç–µ–π
                if not ('<item>' in content_lower or '<entry>' in content_lower):
                    return False
                    
                return True
                
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ {url}: {e}")
            return False
            
    async def _extract_rss_from_github(self) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ RSS-—Å—Å—ã–ª–æ–∫ –∏–∑ GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö RSS-—Å—Å—ã–ª–æ–∫
        """
        rss_urls = set()
        
        for github_url in self.github_sources:
            try:
                logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º RSS-—Ñ–∏–¥—ã –∏–∑ {github_url}")
                
                async with self.session.get(github_url) as response:
                    if response.status == 200:
                        content = await response.text()
                        
                        # –ò—â–µ–º RSS-—Å—Å—ã–ª–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ
                        rss_patterns = [
                            r'https?://[^\s\)]+\.rss',
                            r'https?://[^\s\)]+/feed',
                            r'https?://[^\s\)]+/rss\.xml',
                            r'https?://[^\s\)]+/feed\.xml',
                            r'https?://[^\s\)]+/rss',
                        ]
                        
                        for pattern in rss_patterns:
                            matches = re.findall(pattern, content, re.IGNORECASE)
                            rss_urls.update(matches)
                            
                        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(rss_urls)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö RSS-—Å—Å—ã–ª–æ–∫")
                        
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {github_url}: {e}")
                
        return list(rss_urls)
        
    def _categorize_rss(self, url: str) -> Tuple[Optional[str], Optional[str]]:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è RSS-—Ñ–∏–¥–∞.
        
        Args:
            url: URL RSS-—Ñ–∏–¥–∞
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (category, subcategory) –∏–ª–∏ (None, None)
        """
        url_lower = url.lower()
        domain = urlparse(url).netloc.lower()
        
        for category, subcategories in self.category_keywords.items():
            for subcategory, keywords in subcategories.items():
                for keyword in keywords:
                    if keyword in url_lower or keyword in domain:
                        return category, subcategory
                        
        return None, None
        
    async def _check_existing_sources(self) -> Dict[str, List[str]]:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å –Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        """
        invalid_sources = {}
        all_sources = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        for category, category_data in self.sources_config.items():
            if not isinstance(category_data, dict):
                continue
                
            for subcategory, subcategory_data in category_data.items():
                if not isinstance(subcategory_data, dict):
                    continue
                    
                sources_list = subcategory_data.get('sources', [])
                for source in sources_list:
                    if isinstance(source, dict):
                        url = source.get('url', '')
                        name = source.get('name', '')
                    elif isinstance(source, str):
                        if ':' in source:
                            name, url = source.split(':', 1)
                            name = name.strip()
                            url = url.strip()
                        else:
                            url = source
                            name = ''
                    else:
                        continue
                        
                    if url:
                        all_sources.append((category, subcategory, name, url))
                        
        logger.info(f"–ü—Ä–æ–≤–µ—Ä—è–µ–º {len(all_sources)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
        semaphore = asyncio.Semaphore(5)  # –ú–∞–∫—Å–∏–º—É–º 5 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        
        async def check_source(source_info):
            category, subcategory, name, url = source_info
            async with semaphore:
                is_valid = await self._validate_rss_feed(url)
                self.stats['checked'] += 1
                
                if not is_valid:
                    if category not in invalid_sources:
                        invalid_sources[category] = {}
                    if subcategory not in invalid_sources[category]:
                        invalid_sources[category][subcategory] = []
                    invalid_sources[category][subcategory].append({'name': name, 'url': url})
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º —É–¥–∞–ª–µ–Ω–∏–µ
                    with open('logs/removed_rss.log', 'a', encoding='utf-8') as f:
                        f.write(f"{datetime.now().isoformat()} - {category}/{subcategory} - {url}\n")
                        
                    logger.warning(f"‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π RSS: {category}/{subcategory} - {url}")
                else:
                    logger.info(f"‚úÖ –í–∞–ª–∏–¥–Ω—ã–π RSS: {category}/{subcategory} - {url}")
                    
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        tasks = [check_source(source) for source in all_sources]
        await asyncio.gather(*tasks, return_exceptions=True)
        
        return invalid_sources
        
    async def _remove_invalid_sources(self, invalid_sources: Dict[str, List[str]]):
        """–£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        for category, subcategories in invalid_sources.items():
            if category not in self.sources_config:
                continue
                
            for subcategory, sources in subcategories.items():
                if subcategory not in self.sources_config[category]:
                    continue
                    
                # –£–¥–∞–ª—è–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                valid_sources = []
                sources_list = self.sources_config[category][subcategory].get('sources', [])
                
                for source in sources_list:
                    source_url = ''
                    if isinstance(source, dict):
                        source_url = source.get('url', '')
                    elif isinstance(source, str) and ':' in source:
                        source_url = source.split(':', 1)[1].strip()
                        
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤—Ö–æ–¥–∏—Ç –ª–∏ —ç—Ç–æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ —Å–ø–∏—Å–æ–∫ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö
                    is_invalid = any(
                        inv_source['url'] == source_url 
                        for inv_source in sources
                    )
                    
                    if not is_invalid:
                        valid_sources.append(source)
                        
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                self.sources_config[category][subcategory]['sources'] = valid_sources
                self.stats['removed'] += len(sources)
                self.stats['updated_categories'].add(f"{category}/{subcategory}")
                
                logger.info(f"–£–¥–∞–ª–µ–Ω–æ {len(sources)} –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ {category}/{subcategory}")
                
    async def _add_new_sources(self):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ GitHub."""
        logger.info("–ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–≤—ã–µ RSS-—Ñ–∏–¥—ã –∏–∑ GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤")
        new_rss_urls = await self._extract_rss_from_github()
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(new_rss_urls)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö RSS-—Ñ–∏–¥–æ–≤")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ–º
        valid_new_sources = {}
        
        for url in new_rss_urls:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å
                is_valid = await self._validate_rss_feed(url)
                if not is_valid:
                    continue
                    
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                category, subcategory = self._categorize_rss(url)
                if not category or not subcategory:
                    category, subcategory = 'misc', 'uncategorized'
                    
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∏—Å—Ç–æ—á–Ω–∏–∫ –µ—â–µ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω
                if self._is_source_exists(url):
                    continue
                    
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                if category not in valid_new_sources:
                    valid_new_sources[category] = {}
                if subcategory not in valid_new_sources[category]:
                    valid_new_sources[category][subcategory] = []
                    
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ URL
                name = urlparse(url).netloc.replace('www.', '')
                
                valid_new_sources[category][subcategory].append({
                    'name': name,
                    'url': url
                })
                
                # –õ–æ–≥–∏—Ä—É–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ
                with open('logs/added_rss.log', 'a', encoding='utf-8') as f:
                    f.write(f"{datetime.now().isoformat()} - {category}/{subcategory} - {url}\n")
                    
                logger.info(f"üÜï –ù–æ–≤—ã–π –≤–∞–ª–∏–¥–Ω—ã–π RSS: {category}/{subcategory} - {url}")
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {url}: {e}")
                continue
                
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        for category, subcategories in valid_new_sources.items():
            if category not in self.sources_config:
                self.sources_config[category] = {}
                
            for subcategory, sources in subcategories.items():
                if subcategory not in self.sources_config[category]:
                    self.sources_config[category][subcategory] = {'sources': []}
                    
                # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                for source in sources:
                    self.sources_config[category][subcategory]['sources'].append(source)
                    self.stats['added'] += 1
                    self.stats['updated_categories'].add(f"{category}/{subcategory}")
                    
        logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {self.stats['added']} –Ω–æ–≤—ã—Ö –≤–∞–ª–∏–¥–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        
    def _is_source_exists(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        for category_data in self.sources_config.values():
            if not isinstance(category_data, dict):
                continue
                
            for subcategory_data in category_data.values():
                if not isinstance(subcategory_data, dict):
                    continue
                    
                sources_list = subcategory_data.get('sources', [])
                for source in sources_list:
                    source_url = ''
                    if isinstance(source, dict):
                        source_url = source.get('url', '')
                    elif isinstance(source, str) and ':' in source:
                        source_url = source.split(':', 1)[1].strip()
                        
                    if source_url == url:
                        return True
                        
        return False
        
    async def _update_supabase(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ Supabase."""
        try:
            db_service = get_async_service()
            logger.info("–û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ Supabase")
            
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã sources –≤ Supabase
            # –ü–æ–∫–∞ —á—Ç–æ –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º
            logger.info("Supabase –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è Supabase: {e}")
            
    def _print_summary(self):
        """–í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞."""
        print("\n" + "="*60)
        print("üìä –û–¢–ß–ï–¢ –û–ë–ù–û–í–õ–ï–ù–ò–Ø RSS-–ò–°–¢–û–ß–ù–ò–ö–û–í")
        print("="*60)
        print(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ RSS-—Ñ–∏–¥–æ–≤: {self.stats['checked']}")
        print(f"‚ö†Ô∏è  –£–¥–∞–ª–µ–Ω–æ –Ω–µ—Ä–∞–±–æ—Ç–∞—é—â–∏—Ö: {self.stats['removed']}")
        print(f"üÜï –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö: {self.stats['added']}")
        print(f"üóÇ  –û–±–Ω–æ–≤–ª–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(self.stats['updated_categories'])}")
        
        if self.stats['updated_categories']:
            print(f"\nüìÇ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
            for category in sorted(self.stats['updated_categories']):
                print(f"   ‚Ä¢ {category}")
                
        print("="*60)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –≤ –ª–æ–≥
        with open('logs/rss_update_summary.log', 'w', encoding='utf-8') as f:
            f.write(f"RSS Update Summary - {datetime.now().isoformat()}\n")
            f.write(f"Checked: {self.stats['checked']}\n")
            f.write(f"Removed: {self.stats['removed']}\n")
            f.write(f"Added: {self.stats['added']}\n")
            f.write(f"Updated categories: {len(self.stats['updated_categories'])}\n")
            f.write(f"Categories: {', '.join(sorted(self.stats['updated_categories']))}\n")
            
    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        
        try:
            # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            logger.info("1Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ RSS-—Ñ–∏–¥—ã")
            invalid_sources = await self._check_existing_sources()
            
            # 2. –£–¥–∞–ª—è–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if invalid_sources:
                logger.info("2Ô∏è‚É£ –£–¥–∞–ª—è–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
                await self._remove_invalid_sources(invalid_sources)
            else:
                logger.info("2Ô∏è‚É£ –í—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤–∞–ª–∏–¥–Ω—ã")
                
            # 3. –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ GitHub
            logger.info("3Ô∏è‚É£ –ò—â–µ–º –Ω–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è—Ö")
            await self._add_new_sources()
            
            # 4. –û–±–Ω–æ–≤–ª—è–µ–º Supabase
            logger.info("4Ô∏è‚É£ –û–±–Ω–æ–≤–ª—è–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
            await self._update_supabase()
            
            # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            logger.info("5Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é")
            await self._save_config()
            
            # 6. –í—ã–≤–æ–¥–∏–º –æ—Ç—á–µ—Ç
            self._print_summary()
            
            logger.info("‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è RSS-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")
            raise


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –ª–æ–≥–æ–≤
    Path("logs").mkdir(exist_ok=True)
    
    async with RSSUpdater() as updater:
        await updater.run()


if __name__ == "__main__":
    asyncio.run(main())
