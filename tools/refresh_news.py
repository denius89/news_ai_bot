#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –∑–∞–≥—Ä—É–∑–∫–∏ 500 –Ω–æ–≤—ã—Ö –Ω–∞ —Ä–∞–∑–Ω—ã–µ —Ç–µ–º–∞—Ç–∏–∫–∏
"""

import sys
import os
import logging
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

from database.db_models import supabase, safe_execute
from parsers.rss_parser import parse_source
from services.categories import get_all_sources
from ai_modules.credibility import evaluate_credibility
from ai_modules.importance import evaluate_importance
import random
from datetime import datetime, timezone

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def clear_old_news():
    """–û—á–∏—â–∞–µ—Ç –≤—Å–µ —Å—Ç–∞—Ä—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    logger.info("üóëÔ∏è –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –Ω–æ–≤–æ—Å—Ç–∏...")
    
    try:
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏
        result = safe_execute(
            supabase.table("news").delete().neq("uid", "")
        )
        logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {len(result.data) if result.data else 0} —Å—Ç–∞—Ä—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")
        return True
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
        return False

def load_news_from_sources(target_count=500):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    logger.info(f"üì∞ –ó–∞–≥—Ä—É–∂–∞–µ–º {target_count} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π...")
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ categories service
    try:
        all_sources_data = get_all_sources()
        logger.info(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(all_sources_data)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")
        return 0
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
    random.shuffle(all_sources_data)
    
    loaded_count = 0
    processed_sources = 0
    
    for category, subcategory, source_name, source_url in all_sources_data:
        if loaded_count >= target_count:
            break
            
        try:
            logger.info(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {source_name} ({category}/{subcategory})")
            
            # –ü–∞—Ä—Å–∏–º RSS —Ñ–∏–¥
            news_items = parse_source(
                source_url, 
                category, 
                subcategory, 
                source_name
            )
            
            if not news_items:
                logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –≤ {source_name}")
                continue
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –Ω–æ–≤–æ—Å—Ç—å
            for item in news_items:
                if loaded_count >= target_count:
                    break
                    
                try:
                    # AI –∞–Ω–∞–ª–∏–∑
                    credibility = evaluate_credibility(item)
                    importance = evaluate_importance(item)
                    
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
                    news_data = {
                        'uid': item['uid'],
                        'title': item.get('title', ''),
                        'link': item.get('link', ''),
                        'published_at': item.get('published_at'),
                        'content': item.get('content', ''),
                        'credibility': credibility,
                        'importance': importance,
                        'source': source_name,
                        'category': category,
                        'subcategory': subcategory
                    }
                    
                    # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                    result = safe_execute(
                        supabase.table("news").insert(news_data)
                    )
                    
                    if result.data:
                        loaded_count += 1
                        if loaded_count % 50 == 0:
                            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {loaded_count}/{target_count} –Ω–æ–≤–æ—Å—Ç–µ–π")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
                    continue
            
            processed_sources += 1
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source_name}: {e}")
            continue
    
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {loaded_count} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {processed_sources} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    return loaded_count

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π...")
    
    if not supabase:
        logger.error("‚ùå Supabase –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        return False
    
    # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    if not clear_old_news():
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å —Å—Ç–∞—Ä—ã–µ –Ω–æ–≤–æ—Å—Ç–∏")
        return False
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    loaded_count = load_news_from_sources(500)
    
    if loaded_count > 0:
        logger.info(f"üéâ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {loaded_count} –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π!")
        return True
    else:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
