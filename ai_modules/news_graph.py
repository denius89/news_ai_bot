"""
News Graph System - —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏—Å—Ç–æ—Ä–∏–π.

–°–æ–∑–¥–∞—ë—Ç –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –Ω–æ–≤–æ—Å—Ç—è–º–∏ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤.
"""

import logging
from typing import Dict, List, Any
from datetime import datetime, timedelta, timezone
from collections import Counter

logger = logging.getLogger(__name__)


class NewsGraphBuilder:
    """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –Ω–æ–≤–æ—Å—Ç—è–º–∏."""

    def __init__(self, supabase_client):
        self.supabase = supabase_client

    def find_related_news(
        self, current_news: Dict, lookback_days: int = 30, max_results: int = 5
    ) -> List[Dict[str, Any]]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç —Å–≤—è–∑–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏ —Å–µ–º–∞–Ω—Ç–∏–∫–µ.

        Args:
            current_news: –¢–µ–∫—É—â–∞—è –Ω–æ–≤–æ—Å—Ç—å (—Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏ title, content, etc.)
            lookback_days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π
            start_date = (datetime.now(timezone.utc) - timedelta(days=lookback_days)).isoformat()

            # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ–∫—É—â—É—é –Ω–æ–≤–æ—Å—Ç—å –µ—Å–ª–∏ —É –Ω–µ—ë –µ—Å—Ç—å ID
            query = (
                self.supabase.table("news")
                .select("*")
                .gte("created_at", start_date)
                .order("created_at", desc=True)
                .limit(100)
            )  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

            if current_news.get("id"):
                query = query.neq("id", current_news["id"])

            result = query.execute()
            candidates = result.data or []

            if not candidates:
                logger.info("No news candidates found for graph building")
                return []

            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
            scored_news = []
            current_keywords = self._extract_keywords(current_news)
            current_entities = self._extract_entities(current_news)

            for news_item in candidates:
                similarity_score = self._calculate_similarity(
                    current_news, news_item, current_keywords, current_entities
                )

                if similarity_score > 0.3:  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
                    scored_news.append(
                        {
                            **news_item,
                            "similarity_score": similarity_score,
                            "keywords_overlap": self._get_keywords_overlap(
                                current_keywords, self._extract_keywords(news_item)
                            ),
                            "entities_overlap": self._get_entities_overlap(
                                current_entities, self._extract_entities(news_item)
                            ),
                        }
                    )

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            scored_news.sort(key=lambda x: x["similarity_score"], reverse=True)

            logger.info(f"Found {len(scored_news)} related news items for story context")
            return scored_news[:max_results]

        except Exception as e:
            logger.error(f"Error finding related news: {e}")
            return []

    def create_story_context(
        self, current_news_items: List[Dict], related_history: List[Dict], max_context_items: int = 3
    ) -> str:
        """
        –°–æ–∑–¥–∞—ë—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç 'üìö –†–∞–Ω–µ–µ –º—ã –ø–∏—Å–∞–ª–∏...' –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞.

        Args:
            current_news_items: –¢–µ–∫—É—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –¥–∞–π–¥–∂–µ—Å—Ç–∞
            related_history: –°–≤—è–∑–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
            max_context_items: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Returns:
            –°—Ç—Ä–æ–∫–∞ —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        """
        if not related_history:
            return ""

        # –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏—Å—Ç–æ—Ä–∏–∏
        top_history = sorted(related_history, key=lambda x: x.get("similarity_score", 0), reverse=True)
        top_history = top_history[:max_context_items]

        context_parts = ["üìö –†–∞–Ω–µ–µ –º—ã –ø–∏—Å–∞–ª–∏:"]

        for hist_item in top_history:
            title = hist_item.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")[:80]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            similarity = hist_item.get("similarity_score", 0)
            date = self._format_date(hist_item.get("created_at") or hist_item.get("published_at"))

            context_parts.append(f"‚Ä¢ {title} ({date}, —Å—Ö–æ–∂–µ—Å—Ç—å: {similarity:.2f})")

        context_parts.append("")  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ –∫–æ–Ω—Ü–µ

        logger.info(f"Created story context with {len(top_history)} historical items")
        return "\n".join(context_parts)

    def save_news_links(
        self,
        news_id_1: str,
        news_id_2: str,
        link_type: str = "related",
        similarity_score: float = 0.0,
        keywords_overlap: Dict = None,
        entities_overlap: Dict = None,
    ) -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–≤—è–∑—å –º–µ–∂–¥—É –Ω–æ–≤–æ—Å—Ç—è–º–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.

        Args:
            news_id_1: ID –ø–µ—Ä–≤–æ–π –Ω–æ–≤–æ—Å—Ç–∏
            news_id_2: ID –≤—Ç–æ—Ä–æ–π –Ω–æ–≤–æ—Å—Ç–∏
            link_type: –¢–∏–ø —Å–≤—è–∑–∏
            similarity_score: –û—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
            keywords_overlap: –ü–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–µ—Å—è –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            entities_overlap: –ü–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–µ—Å—è —Å—É—â–Ω–æ—Å—Ç–∏

        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ
        """
        try:
            # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ ID –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
            if news_id_1 > news_id_2:
                news_id_1, news_id_2 = news_id_2, news_id_1

            link_data = {
                "news_id_1": news_id_1,
                "news_id_2": news_id_2,
                "link_type": link_type,
                "similarity_score": similarity_score,
                "keywords_overlap": keywords_overlap or {},
                "entities_overlap": entities_overlap or {},
            }

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º upsert —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            self.supabase.table("news_links").upsert(link_data, on_conflict="news_id_1,news_id_2").execute()

            logger.info(f"Saved news link: {news_id_1} <-> {news_id_2} (score: {similarity_score:.3f})")
            return True

        except Exception as e:
            logger.error(f"Error saving news link: {e}")
            return False

    def _extract_keywords(self, news_item: Dict) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –Ω–æ–≤–æ—Å—Ç–∏."""
        text = f"{news_item.get('title', '')} {news_item.get('content', '')}".lower()

        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å)
        words = text.split()
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        stop_words = {
            "the",
            "and",
            "or",
            "but",
            "in",
            "on",
            "at",
            "to",
            "for",
            "of",
            "with",
            "by",
            "–∞",
            "–∏",
            "–∏–ª–∏",
            "–Ω–æ",
            "–≤",
            "–Ω–∞",
            "–∫",
            "–¥–ª—è",
            "–æ",
            "—Å",
            "–ø–æ",
        }
        keywords = [w for w in words if len(w) > 3 and w not in stop_words]

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
        return [word for word, count in Counter(keywords).most_common(10)]

    def _extract_entities(self, news_item: Dict) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –Ω–æ–≤–æ—Å—Ç–∏."""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å –ø–æ–º–æ—â—å—é NER
        text = f"{news_item.get('title', '')} {news_item.get('content', '')}"

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã –∫–∞–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        import re

        entities = re.findall(r"\b[A-Z–ê-–Ø][a-z–∞-—è]+\b", text)
        return list(set(entities[:10]))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º

    def _calculate_similarity(self, news1: Dict, news2: Dict, keywords1: List[str], entities1: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è –Ω–æ–≤–æ—Å—Ç—è–º–∏."""
        keywords2 = self._extract_keywords(news2)
        entities2 = self._extract_entities(news2)

        # –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        keyword_overlap = len(set(keywords1) & set(keywords2))
        keyword_total = len(set(keywords1) | set(keywords2))
        keyword_sim = keyword_overlap / keyword_total if keyword_total > 0 else 0

        # –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ —Å—É—â–Ω–æ—Å—Ç—è–º
        entity_overlap = len(set(entities1) & set(entities2))
        entity_total = len(set(entities1) | set(entities2))
        entity_sim = entity_overlap / entity_total if entity_total > 0 else 0

        # –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        category_sim = 1.0 if news1.get("category") == news2.get("category") else 0.0

        # –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É
        source_sim = 1.0 if news1.get("source") == news2.get("source") else 0.0

        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
        similarity = keyword_sim * 0.4 + entity_sim * 0.3 + category_sim * 0.2 + source_sim * 0.1

        return min(similarity, 1.0)

    def _get_keywords_overlap(self, keywords1: List[str], keywords2: List[str]) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤."""
        overlap = list(set(keywords1) & set(keywords2))
        return {"overlap_count": len(overlap), "overlap_words": overlap[:5]}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è JSON

    def _get_entities_overlap(self, entities1: List[str], entities2: List[str]) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π."""
        overlap = list(set(entities1) & set(entities2))
        return {"overlap_count": len(overlap), "overlap_entities": overlap[:5]}  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è JSON

    def _format_date(self, date_str: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –¥–∞—Ç—É –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        try:
            if isinstance(date_str, str):
                dt = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
                return dt.strftime("%d.%m")
            return "–Ω–µ–∏–∑–≤."
        except (ValueError, TypeError, AttributeError):
            return "–Ω–µ–∏–∑–≤."


class StoryContextManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏—Å—Ç–æ—Ä–∏–π - –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É —Å –Ω–æ–≤–æ—Å—Ç–Ω—ã–º –≥—Ä–∞—Ñ–æ–º."""

    def __init__(self, supabase_client):
        self.graph_builder = NewsGraphBuilder(supabase_client)

    def get_historical_context_for_digest(self, news_items: List[Dict], category: str, lookback_days: int = 30) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –¥–∞–π–¥–∂–µ—Å—Ç–∞.

        Args:
            news_items: –ù–æ–≤–æ—Å—Ç–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π
            lookback_days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏

        Returns:
            –°—Ç—Ä–æ–∫–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç
        """
        if not news_items:
            return ""

        # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–≤—è–∑–µ–π
        main_news = max(news_items, key=lambda x: x.get("importance", 0))

        # –ù–∞—Ö–æ–¥–∏–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        related_news = self.graph_builder.find_related_news(main_news, lookback_days=lookback_days, max_results=5)

        # –°–æ–∑–¥–∞—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = self.graph_builder.create_story_context(news_items, related_news, max_context_items=3)

        return context
