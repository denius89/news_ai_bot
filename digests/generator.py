# digests/generator.py
"""–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤: –æ–±—ã—á–Ω—ã—Ö –∏ AI.

- fetch_recent_news: –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ Supabase.
- generate_digest: —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ HTML-–¥–∞–π–¥–∂–µ—Å—Ç–∞ (AI/–Ω–µ-AI).
"""

import argparse
import logging
from datetime import datetime
from typing import Optional, List, Dict

from database.db_models import supabase
from digests.ai_summary import generate_batch_summary

logger = logging.getLogger("generator")


def fetch_recent_news(limit: int = 10, category: Optional[str] = None) -> List[Dict]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –ë–î (Supabase).

    Args:
        limit: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –Ω–æ–≤–æ—Å—Ç–µ–π.
        category: –∫–∞—Ç–µ–≥–æ—Ä–∏—è –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–≤ –ë–î —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ lowercase).

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏.
    """
    if not supabase:
        logger.warning("‚ö†Ô∏è Supabase –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π.")
        return []

    query = (
        supabase.table("news")
        .select("id, title, content, link, importance, published_at, source, category")
        .order("importance", desc=True)
        .order("published_at", desc=True)
        .limit(limit)
    )
    if category:
        query = query.eq("category", category.lower())

    response = query.execute()
    rows = response.data or []

    for row in rows:
        published_at_fmt = "‚Äî"
        if row.get("published_at"):
            try:
                dt = datetime.fromisoformat(row["published_at"].replace("Z", "+00:00"))
                published_at_fmt = dt.strftime("%d %b %Y, %H:%M")
            except Exception:
                pass
        row["published_at_fmt"] = published_at_fmt

    return rows


def generate_digest(
    limit: int = 10,
    ai: bool = False,
    category: Optional[str] = None,
    style: str = "analytical",
) -> str:
    """–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –¥–∞–π–¥–∂–µ—Å—Ç.

    –ï—Å–ª–∏ `ai=True` ‚Äî –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è LLM-—Å–≤–æ–¥–∫–∞ –ø–æ —Å–ø–∏—Å–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π.
    –ò–Ω–∞—á–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ–π HTML-—Å–ø–∏—Å–æ–∫.
    """
    # –¥–ª—è AI-–¥–∞–π–¥–∂–µ—Å—Ç–∞ –±–µ—Ä–µ–º –±–æ–ª—å—à–µ –Ω–æ–≤–æ—Å—Ç–µ–π
    if ai and limit < 15:
        limit = 15

    news_items = fetch_recent_news(limit=limit, category=category)
    if not news_items:
        return "–°–µ–≥–æ–¥–Ω—è –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç."

    if ai:
        summary_text = generate_batch_summary(news_items, style=style) or ""
        if "<b>–ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ" not in summary_text:
            summary_text += (
                "\n\n<b>–ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ:</b>\n"
                "‚Äî –°–æ–±—ã—Ç–∏–µ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä—ã–Ω–æ–∫\n"
                "‚Äî –í–∞–∂–Ω–æ –¥–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤\n"
                "‚Äî –ú–æ–∂–µ—Ç –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫–æ–º–ø–∞–Ω–∏–π"
            )
        return summary_text.strip()

    # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π (–±–µ–∑ AI)
    lines = []
    for i, item in enumerate(news_items, 1):
        title = item.get("title", "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
        date = item.get("published_at_fmt", "‚Äî")
        link = item.get("link")
        if link:
            lines.append(f'{i}. <b>{title}</b> [{date}] ‚Äî <a href="{link}">–ü–æ–¥—Ä–æ–±–Ω–µ–µ</a>')
        else:
            lines.append(f"{i}. <b>{title}</b> [{date}]")

    return "üì∞ <b>–î–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π:</b>\n\n" + "\n".join(lines)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--ai", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AI –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞")
    parser.add_argument("--limit", type=int, default=10, help="–°–∫–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤–∫–ª—é—á–∞—Ç—å")
    parser.add_argument("--category", type=str, help="–§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (crypto, economy, ...)")
    parser.add_argument(
        "--style", type=str, default="analytical", choices=["analytical", "business", "meme"]
    )
    args = parser.parse_args()

    print(generate_digest(limit=args.limit, ai=args.ai, category=args.category, style=args.style))
