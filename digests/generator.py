# digests/generator.py
import argparse
import logging
from datetime import datetime
from typing import Optional, List, Dict

from database.db_models import supabase
from digests.ai_summary import generate_batch_summary  # batch-–∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –∏–∑ ai_summary

logger = logging.getLogger("generator")


def fetch_recent_news(limit: int = 10, category: Optional[str] = None) -> List[Dict]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –±–∞–∑—ã (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ importance –∏ –¥–∞—Ç–µ).
    –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ category ‚Äî —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Å—Ä–∞–∑—É –≤ –∑–∞–ø—Ä–æ—Å–µ.
    –î–æ–±–∞–≤–ª—è–µ—Ç –ø–æ–ª–µ published_at_fmt –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.
    """
    if not supabase:
        logger.warning("‚ö†Ô∏è Supabase –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π.")
        return []

    query = (
        supabase.table("news")
        .select("id, title, content, link, importance, published_at, source, category")
        .order("importance", desc=True)
        .order("published_at", desc=True)
        .limit(limit)
    )

    if category:
        query = query.eq("category", category)

    response = query.execute()
    rows = response.data or []
    news = []

    for row in rows:
        published_at_fmt = "‚Äî"
        if row.get("published_at"):
            try:
                dt = datetime.fromisoformat(row["published_at"].replace("Z", "+00:00"))
                published_at_fmt = dt.strftime("%d %b %Y, %H:%M")
            except Exception:
                pass

        row["published_at_fmt"] = published_at_fmt
        news.append(row)

    return news


def fetch_news_by_date_range(
    start: datetime, end: datetime, category: Optional[str] = None, limit: int = 30
) -> List[Dict]:
    """
    –î–æ—Å—Ç–∞—ë–º –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç.
    """
    if not supabase:
        logger.warning("‚ö†Ô∏è Supabase –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π.")
        return []

    query = (
        supabase.table("news")
        .select("id, title, content, link, importance, published_at, source, category")
        .gte("published_at", start.isoformat())
        .lte("published_at", end.isoformat())
        .order("published_at", desc=True)
        .limit(limit)
    )

    if category:
        query = query.eq("category", category)

    response = query.execute()
    rows = response.data or []
    news = []

    for row in rows:
        published_at_fmt = "‚Äî"
        if row.get("published_at"):
            try:
                dt = datetime.fromisoformat(row["published_at"].replace("Z", "+00:00"))
                published_at_fmt = dt.strftime("%d %b %Y, %H:%M")
            except Exception:
                pass
        row["published_at_fmt"] = published_at_fmt
        news.append(row)

    return news


def generate_digest(
    limit: int = 10,
    ai: bool = False,
    category: Optional[str] = None,
    start: Optional[datetime] = None,
    end: Optional[datetime] = None,
) -> str:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–∞–π–¥–∂–µ—Å—Ç–∞:
    - –µ—Å–ª–∏ ai=True ‚Üí —Å–≤—è–∑–Ω—ã–π AI-summary (batch)
    - –∏–Ω–∞—á–µ ‚Üí —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
    - –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç ‚Üí –±–µ—Ä—ë–º fetch_news_by_date_range
    - –∏–Ω–∞—á–µ ‚Üí fetch_recent_news
    """
    if ai and limit < 15:
        limit = 15

    if start and end:
        news_items = fetch_news_by_date_range(start, end, category=category, limit=limit)
    else:
        news_items = fetch_recent_news(limit=limit, category=category)

    if not news_items:
        return "–°–µ–≥–æ–¥–Ω—è –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç."

    if ai:
        summary_text = generate_batch_summary(news_items)
        if not summary_text:
            return "‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI-–¥–∞–π–¥–∂–µ—Å—Ç–∞."
        return summary_text  # ‚ö° HTML-—Ñ–æ—Ä–º–∞—Ç –¥–ª—è Telegram

    # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç –±–µ–∑ AI
    lines = []
    for i, item in enumerate(news_items, 1):
        title = item.get("title", "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
        date = item.get("published_at_fmt", "‚Äî")
        link = item.get("link")
        if link:
            lines.append(f"{i}. <b>{title}</b> [{date}] ‚Äî <a href=\"{link}\">–ü–æ–¥—Ä–æ–±–Ω–µ–µ</a>")
        else:
            lines.append(f"{i}. <b>{title}</b> [{date}]")

    digest_text = "üì∞ <b>–î–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π:</b>\n\n" + "\n".join(lines)
    return digest_text


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--ai", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AI –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞")
    parser.add_argument("--limit", type=int, default=10, help="–°–∫–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤–∫–ª—é—á–∞—Ç—å")
    parser.add_argument("--category", type=str, help="–§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (crypto, economy, ...)")
    parser.add_argument("--start", type=str, help="–î–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ (ISO)")
    parser.add_argument("--end", type=str, help="–î–∞—Ç–∞ –∫–æ–Ω—Ü–∞ (ISO)")
    args = parser.parse_args()

    start = datetime.fromisoformat(args.start) if args.start else None
    end = datetime.fromisoformat(args.end) if args.end else None

    print(
        generate_digest(limit=args.limit, ai=args.ai, category=args.category, start=start, end=end)
    )
