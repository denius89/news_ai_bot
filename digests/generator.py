# digests/generator.py
import argparse
import logging
from datetime import datetime
from typing import Optional, List, Dict

from database.db_models import supabase
from digests.ai_summary import generate_batch_summary

logger = logging.getLogger("generator")


def fetch_recent_news(limit: int = 10, category: Optional[str] = None) -> List[Dict]:
    """–ü–æ–ª—É—á–∞–µ–º —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –ë–î (supabase)."""
    if not supabase:
        logger.warning("‚ö†Ô∏è Supabase –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π.")
        return []

    query = (
        supabase.table("news")
        .select("id, title, content, link, importance, published_at, source, category")
        .order("importance", desc=True)
        .order("published_at", desc=True)
        .limit(limit)
    )
    if category:
        query = query.eq("category", category)

    response = query.execute()
    rows = response.data or []
    news: List[Dict] = []

    for row in rows:
        published_at_fmt = "‚Äî"
        if row.get("published_at"):
            try:
                dt = datetime.fromisoformat(row["published_at"].replace("Z", "+00:00"))
                published_at_fmt = dt.strftime("%d %b %Y, %H:%M")
            except Exception:
                pass
        row["published_at_fmt"] = published_at_fmt
        news.append(row)

    return news


def generate_digest(
    limit: int = 10,
    ai: bool = False,
    category: Optional[str] = None,
    style: str = "analytical",  # ‚úÖ —Å—Ç–∏–ª—å –ø—Ä–æ–∫–∏–¥—ã–≤–∞–µ–º —Å—é–¥–∞
) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–π–¥–∂–µ—Å—Ç–∞ (AI –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π)."""
    # –¥–ª—è AI-–¥–∞–π–¥–∂–µ—Å—Ç–∞ –±–µ—Ä—ë–º –±–æ–ª—å—à–µ –Ω–æ–≤–æ—Å—Ç–µ–π
    if ai and limit < 15:
        limit = 15

    news_items = fetch_recent_news(limit=limit, category=category)
    if not news_items:
        return "–°–µ–≥–æ–¥–Ω—è –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç."

    if ai:
        summary_text = generate_batch_summary(news_items, style=style)
        if not summary_text or summary_text.strip() == "":
            return "‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI-–¥–∞–π–¥–∂–µ—Å—Ç–∞."

        # üö® fallback: –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –±–ª–æ–∫ ¬´–ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ¬ª
        if "<b>–ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ" not in summary_text:
            summary_text += (
                "\n\n<b>–ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ:</b>\n"
                "‚Äî –°–æ–±—ã—Ç–∏–µ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä—ã–Ω–æ–∫\n"
                "‚Äî –í–∞–∂–Ω–æ –¥–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤\n"
                "‚Äî –ú–æ–∂–µ—Ç –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫–æ–º–ø–∞–Ω–∏–π"
            )

        return summary_text.strip()  # HTML-—Ñ–æ—Ä–º–∞—Ç

    # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π (–±–µ–∑ AI)
    lines = []
    for i, item in enumerate(news_items, 1):
        title = item.get("title", "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
        date = item.get("published_at_fmt", "‚Äî")
        link = item.get("link")
        if link:
            lines.append(f'{i}. <b>{title}</b> [{date}] ‚Äî <a href="{link}">–ü–æ–¥—Ä–æ–±–Ω–µ–µ</a>')
        else:
            lines.append(f"{i}. <b>{title}</b> [{date}]")

    return "üì∞ <b>–î–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π:</b>\n\n" + "\n".join(lines)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--ai", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AI –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞")
    parser.add_argument("--limit", type=int, default=10, help="–°–∫–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤–∫–ª—é—á–∞—Ç—å")
    parser.add_argument("--category", type=str, help="–§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (crypto, economy, ...)")
    parser.add_argument(
        "--style", type=str, default="analytical", choices=["analytical", "business", "meme"]
    )
    args = parser.parse_args()

    print(generate_digest(limit=args.limit, ai=args.ai, category=args.category, style=args.style))
