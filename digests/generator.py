import argparse
import logging
from datetime import datetime

from database.db_models import supabase
from digests.ai_summary import generate_batch_summary  # batch-–∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –∏–∑ ai_summary

logger = logging.getLogger("generator")


def fetch_recent_news(limit: int = 10, category: str | None = None):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –±–∞–∑—ã (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ importance –∏ –¥–∞—Ç–µ).
    –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ category ‚Äî —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Å—Ä–∞–∑—É –≤ –∑–∞–ø—Ä–æ—Å–µ.
    –î–æ–±–∞–≤–ª—è–µ—Ç –ø–æ–ª–µ published_at_fmt –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.
    """
    if not supabase:
        logger.warning("‚ö†Ô∏è Supabase –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π.")
        return []

    query = (
        supabase.table("news")
        .select("id, title, content, link, importance, published_at, source, category")
        .order("importance", desc=True)
        .order("published_at", desc=True)
        .limit(limit)
    )

    if category:
        query = query.eq("category", category)

    response = query.execute()
    rows = response.data or []
    news = []

    for row in rows:
        # —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞—Ç—É
        published_at_fmt = "‚Äî"
        if row.get("published_at"):
            try:
                dt = datetime.fromisoformat(row["published_at"].replace("Z", "+00:00"))
                published_at_fmt = dt.strftime("%d %b %Y, %H:%M")
            except Exception:
                pass

        row["published_at_fmt"] = published_at_fmt
        news.append(row)

    return news


def generate_digest(limit: int = 10, ai: bool = False, category: str | None = None) -> str:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–∞–π–¥–∂–µ—Å—Ç–∞:
    - –µ—Å–ª–∏ ai=True ‚Üí —Å–≤—è–∑–Ω—ã–π AI-summary (batch)
    - –∏–Ω–∞—á–µ ‚Üí —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
    - –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ category ‚Üí —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Å—Ä–∞–∑—É –≤ –∑–∞–ø—Ä–æ—Å–µ
    """
    # –¥–ª—è AI-–¥–∞–π–¥–∂–µ—Å—Ç–∞ –±–µ—Ä—ë–º –±–æ–ª—å—à–µ –Ω–æ–≤–æ—Å—Ç–µ–π
    if ai and limit < 15:
        limit = 15

    news_items = fetch_recent_news(limit=limit, category=category)

    if not news_items:
        return "–°–µ–≥–æ–¥–Ω—è –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç."

    if ai:
        summary_text = generate_batch_summary(news_items)
        if not summary_text:
            return "‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI-–¥–∞–π–¥–∂–µ—Å—Ç–∞."
        return summary_text  # ‚ö° HTML-—Ñ–æ—Ä–º–∞—Ç –¥–ª—è Telegram

    # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç –±–µ–∑ AI
    lines = []
    for i, item in enumerate(news_items, 1):
        title = item.get("title", "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
        link = item.get("link", "")
        date = item.get("published_at_fmt", "‚Äî")
        lines.append(f"{i}. <b>{title}</b> [{date}] ‚Äî <a href=\"{link}\">–ü–æ–¥—Ä–æ–±–Ω–µ–µ</a>")

    digest_text = "üì∞ <b>–î–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π:</b>\n\n" + "\n".join(lines)
    return digest_text


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--ai", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AI –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞")
    parser.add_argument("--limit", type=int, default=10, help="–°–∫–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤–∫–ª—é—á–∞—Ç—å")
    parser.add_argument("--category", type=str, help="–§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (crypto, economy, ...)")
    args = parser.parse_args()

    print(generate_digest(limit=args.limit, ai=args.ai, category=args.category))
