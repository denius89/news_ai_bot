import argparse
import logging
from database.db_models import supabase
from digests.ai_summary import generate_summary  # –º–æ–¥—É–ª—å AI-—Å–∞–º–º–∞—Ä–∏

logger = logging.getLogger(__name__)

def fetch_recent_news(limit: int = 5):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –±–∞–∑—ã (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ importance –∏ –¥–∞—Ç–µ).
    """
    response = supabase.table("news") \
        .select("id, title, content, link, importance, published_at") \
        .order("importance", desc=True) \
        .order("published_at", desc=True) \
        .limit(limit) \
        .execute()

    return response.data or []

def generate_digest(limit: int = 5, ai: bool = False) -> str:
    """
    –§–æ—Ä–º–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–∞–π–¥–∂–µ—Å—Ç–∞:
    - –µ—Å–ª–∏ ai=True ‚Üí –¥–µ–ª–∞–µ–º AI-summary
    - –∏–Ω–∞—á–µ ‚Üí —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
    """
    news_items = fetch_recent_news(limit=limit)
    if not news_items:
        return "–ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –¥–∞–π–¥–∂–µ—Å—Ç–∞."

    if ai:
        return generate_summary(news_items)

    # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –¥–∞–π–¥–∂–µ—Å—Ç
    lines = []
    for i, item in enumerate(news_items, 1):
        title = item.get("title", "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
        link = item.get("link", "")
        lines.append(f"{i}. {title} ({link})")

    digest_text = "üì∞ –î–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π:\n\n" + "\n".join(lines)
    return digest_text


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--ai", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AI –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–π–¥–∂–µ—Å—Ç–∞")
    parser.add_argument("--limit", type=int, default=5, help="–°–∫–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤–∫–ª—é—á–∞—Ç—å")
    args = parser.parse_args()

    print(generate_digest(limit=args.limit, ai=args.ai))