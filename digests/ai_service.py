"""
AI Digest Service - centralized AI-powered digest generation.

This module contains the main DigestAIService class that handles:
- News digest generation with AI analysis
- Fallback to simple format when OpenAI API is not available
- Date formatting and news limiting (max 8 items)
"""

import logging
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
from datetime import datetime, timedelta

from models.news import NewsItem
from utils.text.formatters import format_date
from utils.ai.ai_client import ask_async
from digests.prompts import get_prompt_for_category, PROMPTS as LEGACY_PROMPTS

try:
    from digests.prompts_v2 import build_prompt, STYLE_CARDS, CATEGORY_CARDS

    PROMPTS_V2_AVAILABLE = True
except ImportError:
    PROMPTS_V2_AVAILABLE = False

try:
    from digests.multistage_generator import generate_multistage_digest

    MULTISTAGE_AVAILABLE = True
except ImportError:
    MULTISTAGE_AVAILABLE = False

try:
    from digests.rag_system import get_rag_context

    RAG_SYSTEM_AVAILABLE = True
except ImportError:
    RAG_SYSTEM_AVAILABLE = False

try:
    from digests.personalization import PersonalizedDigestGenerator

    PERSONALIZATION_AVAILABLE = True
except ImportError:
    PERSONALIZATION_AVAILABLE = False

try:
    from ai_modules.personas import select_persona_for_context
    from ai_modules.news_graph import StoryContextManager
    from ai_modules.feedback_loop import FeedbackAnalyzer

    SUPER_JOURNALIST_V3_AVAILABLE = True
except ImportError:
    SUPER_JOURNALIST_V3_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class DigestConfig:
    """Configuration for digest generation."""

    max_items: int = 8
    include_fallback: bool = True
    style: str = "analytical"
    use_multistage: bool = False  # Enable multi-stage generation with Chain-of-Thought
    use_rag: bool = True  # Enable RAG system with high-quality examples
    use_personalization: bool = True  # Enable personalization based on user profile
    use_events: bool = True  # Enable events fetching from database
    user_id: Optional[str] = None  # User ID for personalization
    audience: str = "general"  # Target audience type

    # Super Journalist v3 features
    use_personas: bool = False  # Enable automatic persona selection
    use_story_memory: bool = False  # Enable historical context from news graph
    use_feedback_loop: bool = False  # Enable feedback-based improvements


class DigestAIService:
    """
    AI-powered digest generation service.

    Handles both AI-enhanced and fallback digest generation.
    """

    def __init__(self, config: Optional[DigestConfig] = None):
        self.config = config or DigestConfig()
        self._openai_available = self._check_openai_availability()

    def _check_openai_availability(self) -> bool:
        """Check if OpenAI API is available."""
        try:
            import os

            api_key = os.getenv("OPENAI_API_KEY")
            return bool(api_key)
        except Exception:
            return False

    def _get_max_tokens_for_length(self, length: str) -> int:
        """Calculate max_tokens based on length parameter."""
        if length == "short":
            return 500  # ~300 words
        elif length == "medium":
            return 1000  # ~600 words
        elif length == "long":
            return 2000  # ~1000 words
        else:
            return 1000  # default

    async def build_digest(
        self,
        news_items: List[NewsItem],
        style: str = "analytical",
        category: str = "all",
        length: str = "medium",
        subcategory: Optional[str] = None,
    ) -> str:
        """
        Build AI-powered digest from news items.

        Args:
            news_items: List of NewsItem objects
            style: Digest style (analytical, business, meme)
            category: News category (crypto, sports, markets, tech, world)
            length: Text length (short, medium, long)

        Returns:
            Formatted digest string
        """
        if not news_items:
            return self._build_empty_digest()

        # Limit to max_items
        limited_news = news_items[: self.config.max_items]

        if self._openai_available:
            # Don't catch exceptions, let them propagate (including timeout)
            return await self._llm_summarize(limited_news, style, category, length, subcategory)
        else:
            logger.info("OpenAI API not available, using fallback digest")
            return self._build_fallback_digest(limited_news)

    async def _llm_summarize(
        self,
        news_items: List[NewsItem],
        style: str,
        category: str = "world",
        length: str = "medium",
        subcategory: Optional[str] = None,
    ) -> str:
        """
        Generate AI-powered summary using OpenAI.

        Args:
            news_items: List of news items to summarize
            style: Summary style
            category: News category for context
            length: Text length (short, medium, long)

        Returns:
            AI-generated digest text
        """
        # Prepare news data for AI
        news_data = []
        for item in news_items:
            news_data.append(
                {
                    "title": item.title,
                    "content": item.content or "",
                    "published_at": item.published_at_fmt if item.published_at else "Unknown",
                    "source": item.source or "Unknown",
                    "credibility": item.credibility or 0.0,
                    "importance": item.importance or 0.0,
                    "subcategory": item.subcategory,  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é
                }
            )

        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—É—é –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π
        subcategory = None
        if news_data:
            from collections import Counter

            subcats = [item.get("subcategory") for item in news_data if item.get("subcategory")]
            if subcats:
                subcategory = Counter(subcats).most_common(1)[0][0]

        # –ü–æ–ª—É—á–∏—Ç—å —Å–æ–±—ã—Ç–∏—è —Å —É—á–µ—Ç–æ–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        events = await self._fetch_relevant_events(news_items, category, subcategory)

        # Try multi-stage generation if enabled and available
        if self.config.use_multistage and MULTISTAGE_AVAILABLE:
            try:
                logger.info("Using multi-stage generation with Chain-of-Thought")
                result = await generate_multistage_digest(
                    news_items=news_items,
                    category=category,
                    subcategory=subcategory,
                    style=style,
                    events=events,
                    use_reasoning=True,
                    use_rag=self.config.use_rag,
                )

                logger.info(
                    f"Multi-stage generation completed: {result['stats']['word_count']} words, {result['stats']['generation_time_sec']:.2f}s"
                )
                return result["text"]

            except Exception as e:
                logger.warning(f"Multi-stage generation failed, falling back to standard: {e}")

        # Create prompt based on style and category
        prompt = await self._create_prompt(news_data, events, style, category, length, subcategory, news_items)
        logger.info(f"Created prompt length: {len(prompt)}")
        logger.info(f"News data count: {len(news_data)}")

        # Calculate max_tokens and timeout based on length
        max_tokens = self._get_max_tokens_for_length(length)
        timeout_seconds = 30.0 if length == "long" else 15.0  # Longer timeout for long digests
        logger.info(f"Using max_tokens={max_tokens}, timeout={timeout_seconds}s for length={length}")

        # Call OpenAI API with timeout
        import asyncio

        try:
            response = await asyncio.wait_for(
                ask_async(prompt=prompt, style=style, max_tokens=max_tokens), timeout=timeout_seconds
            )
        except asyncio.TimeoutError:
            logger.error(f"OpenAI API timeout after {timeout_seconds} seconds for length={length}")
            raise  # Re-raise the timeout error instead of using fallback

        logger.info(f"AI response length: {len(response) if response else 0}")
        logger.info(f"AI response preview: {response[:200] if response else 'EMPTY'}")

        # Process response - check if it's JSON and convert to HTML
        if response:
            # Import JSON formatter
            from digests.json_formatter import format_json_digest_to_html, clean_json_from_text

            # Check if response is JSON
            if response.strip().startswith("{") and response.strip().endswith("}"):
                logger.info("Converting JSON response to HTML")
                response = format_json_digest_to_html(response)
            else:
                # Clean any JSON blocks from text
                response = clean_json_from_text(response)

            # Clean HTML containers if AI generated them despite instructions
            import re

            # Remove HTML document structure
            response = re.sub(r"<!DOCTYPE[^>]*>", "", response, flags=re.IGNORECASE)
            response = re.sub(r"<html[^>]*>", "", response, flags=re.IGNORECASE)
            response = re.sub(r"</html>", "", response, flags=re.IGNORECASE)
            response = re.sub(r"<head[^>]*>.*?</head>", "", response, flags=re.DOTALL | re.IGNORECASE)
            response = re.sub(r"<body[^>]*>", "", response, flags=re.IGNORECASE)
            response = re.sub(r"</body>", "", response, flags=re.IGNORECASE)
            response = re.sub(r"<style[^>]*>.*?</style>", "", response, flags=re.DOTALL | re.IGNORECASE)
            # Remove problematic containers
            response = re.sub(r"<div[^>]*>", "", response, flags=re.IGNORECASE)
            response = re.sub(r"</div>", "", response, flags=re.IGNORECASE)
            response = re.sub(r"<p[^>]*>", "", response, flags=re.IGNORECASE)
            response = re.sub(r"</p>", "", response, flags=re.IGNORECASE)
            response = re.sub(r"<span[^>]*>", "", response, flags=re.IGNORECASE)
            response = re.sub(r"</span>", "", response, flags=re.IGNORECASE)
            response = response.strip()

        # Don't add fallback section - let AI handle it naturally

        return response

    async def _fetch_relevant_events(
        self, news_items: List[NewsItem], category: str, subcategory: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –∏–∑ –ë–î."""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫
            if not self.config.use_events:
                logger.info("Events disabled by config")
                return []

            # –ò–º–ø–æ—Ä—Ç —Å–µ—Ä–≤–∏—Å–∞ —Å–æ–±—ã—Ç–∏–π
            from database.events_service import get_events_service

            events_service = get_events_service()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–∏–æ–¥
            from datetime import timezone
            import time

            now = datetime.now(timezone.utc)
            start = now - timedelta(hours=12)
            end = now + timedelta(days=2)

            # ‚úÖ –ß–ò–¢–ê–ï–ú –ò–ó –ë–î (–±—ã—Å—Ç—Ä–æ!)
            start_time = time.time()
            all_events = await events_service.get_events_by_date_range(
                from_date=start, to_date=end, category=category  # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ä–∞–∑—É
            )
            elapsed = time.time() - start_time

            logger.info(f"‚è±Ô∏è Events from DB: {len(all_events)} events " f"in {elapsed*1000:.0f}ms for {category}")

            if not all_events:
                logger.info("No events found in database for period")
                return []

            # –§–∏–ª—å—Ç—Ä –ø–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            if subcategory:
                all_events = [e for e in all_events if e.subcategory == subcategory]
                logger.info(f"After subcategory filter: {len(all_events)}")

            # –§–∏–ª—å—Ç—Ä –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            relevant = [e for e in all_events if e.importance > 0.65]

            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ —Ç–æ–ø-4
            relevant.sort(key=lambda x: x.importance, reverse=True)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = []
            for e in relevant[:4]:
                result.append(
                    {
                        "title": e.title,
                        "date": e.starts_at.strftime("%d.%m.%Y"),
                        "description": e.description or "",
                        "importance": e.importance,
                        "subcategory": e.subcategory,
                    }
                )

            return result

        except Exception as e:
            logger.warning(f"Failed to fetch events: {e}")
            return []  # Graceful degradation

    async def _create_prompt(
        self,
        news_data: List[Dict[str, Any]],
        events: List[Dict[str, Any]],
        style: str,
        category: str = "world",
        length: str = "medium",
        subcategory: Optional[str] = None,
        news_items: Optional[List[NewsItem]] = None,
    ) -> str:
        """Create AI prompt based on news data, style, category, events, and RAG examples."""

        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        news_text = "\n\n".join(
            [
                (
                    f"{item['title'][:150]}...\n"  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    f"{item['published_at']} | {item['source']}\n"
                    f"–î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å: {item['credibility']:.1f} | –í–∞–∂–Ω–æ—Å—Ç—å: {item['importance']:.1f}\n"
                    f"{item['content'][:150]}..."  # –°–æ–∫—Ä–∞—â–∞–µ–º —Å 200 –¥–æ 150 —Å–∏–º–≤–æ–ª–æ–≤
                    if item["content"]
                    else "–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"
                )
                for item in news_data[:6]  # –ú–∞–∫—Å–∏–º—É–º 6 –Ω–æ–≤–æ—Å—Ç–µ–π –≤–º–µ—Å—Ç–æ –≤—Å–µ—Ö
            ]
        )

        # –î–æ–±–∞–≤–ª—è–µ–º RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
        rag_context = ""
        if self.config.use_rag and RAG_SYSTEM_AVAILABLE and news_items:
            try:
                rag_context = get_rag_context(
                    category=category,
                    subcategory=subcategory,
                    style=style,
                    news_items=news_items,
                    max_samples=3,  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫ 3 –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞, –Ω–æ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º —ç—Ç–æ –±—ã—Å—Ç—Ä–æ
                )
                if rag_context:
                    # –†–∞–∑—É–º–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏/–∫–∞—á–µ—Å—Ç–≤–∞
                    if len(rag_context) > 3000:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å 2000 –¥–æ 3000 –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
                        rag_context = rag_context[:3000] + "..."
                    news_text = rag_context + "\n\n" + news_text
                    logger.info(f"Added RAG context: {len(rag_context)} characters")
            except Exception as e:
                logger.warning(f"Failed to add RAG context: {e}")

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        personalization_context = ""
        personalized_style = style
        personalized_tone = "neutral"
        personalized_audience = self.config.audience

        if self.config.use_personalization and PERSONALIZATION_AVAILABLE:
            try:
                # –£–ú–ù–ê–Ø –ü–ï–†–°–û–ù–ê–õ–ò–ó–ê–¶–ò–Ø: –±–µ–∑ —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤, –Ω–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–∞—á–µ—Å—Ç–≤–∞
                if self.config.audience in ["pro", "expert"]:
                    # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—ã –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª–µ–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥
                    if style not in ["analytical", "business", "technical"]:
                        personalized_style = "analytical"
                    else:
                        personalized_style = style
                    personalized_tone = "formal"
                    personalization_context = "–ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é –∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑."
                elif self.config.audience in ["beginner", "casual"]:
                    # –ù–æ–≤–∏—á–∫–∏ –ø–æ–ª—É—á–∞—é—Ç –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥
                    if style not in ["casual", "magazine"]:
                        personalized_style = "casual"
                    else:
                        personalized_style = style
                    personalized_tone = "friendly"
                    personalization_context = "–û–±—ä—è—Å–Ω—è–π —Å–ª–æ–∂–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏."
                else:  # general
                    personalized_style = style
                    personalized_tone = "neutral"
                    personalization_context = "–ò—Å–ø–æ–ª—å–∑—É–π —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å—Ç–∏–ª—å –¥–ª—è —à–∏—Ä–æ–∫–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏."

                logger.info(f"Enhanced personalization - audience: {self.config.audience}, style: {personalized_style}")

            except Exception as e:
                logger.warning(f"Failed to add personalization, using defaults: {e}")
                personalized_style = style
                personalization_context = ""

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º —Å—Ç–∏–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if personalized_style not in STYLE_CARDS:
            logger.warning(f"Invalid personalized_style '{personalized_style}', falling back to '{style}'")
            logger.warning(f"Available styles: {list(STYLE_CARDS.keys())}")
            personalized_style = style

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∏–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º analytical
        if personalized_style not in STYLE_CARDS:
            logger.warning(f"Original style '{style}' also not found in STYLE_CARDS, using 'analytical' as fallback")
            personalized_style = "analytical"

        logger.info(
            f"Style check: original='{style}', personalized='{personalized_style}', valid={personalized_style in STYLE_CARDS}"
        )

        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ —Ñ—É–Ω–∫—Ü–∏—è story memory
        story_context = ""
        if self.config.use_story_memory and SUPER_JOURNALIST_V3_AVAILABLE and news_items:
            try:
                from database.db_models import supabase

                if supabase:
                    context_manager = StoryContextManager(supabase)
                    story_context = context_manager.get_historical_context_for_digest(
                        news_items=[
                            {
                                "id": item.get("id"),
                                "title": item.get("title", ""),
                                "content": item.get("content", ""),
                                "importance": item.get("importance", 0.5),
                                "category": category,
                            }
                            for item in news_items
                        ],
                        category=category,
                        lookback_days=30,
                    )
                    if story_context:
                        news_text = story_context + "\n\n" + news_text
                        logger.info(f"Added historical context: {len(story_context)} characters")
            except Exception as e:
                logger.warning(f"Failed to add story context: {e}")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É prompts_v2 –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞ –∏ —Å—Ç–∏–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        if PROMPTS_V2_AVAILABLE and personalized_style in STYLE_CARDS and category in CATEGORY_CARDS:
            logger.info(f"Using prompts_v2 for style: {style}, category: {category}, subcategory: {subcategory}")

            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏—è –∫ news_text –µ—Å–ª–∏ –µ—Å—Ç—å
            if events:
                events_text = "\n\nüìÖ –ü–†–ï–î–°–¢–û–Ø–©–ò–ï –°–û–ë–´–¢–ò–Ø:\n" + "\n".join(
                    [
                        (
                            f"‚Ä¢ {e['title']} ({e['date']}) ‚Äî –≤–∞–∂–Ω–æ—Å—Ç—å: {e['importance']:.1f}"
                            f"\n  –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è: {e['subcategory']}"
                            f"\n  {e['description']}"
                            if e["description"]
                            else ""
                        )
                        for e in events
                    ]
                )
                news_text += events_text + "\n\n–ï—Å–ª–∏ —Å–æ–±—ã—Ç–∏—è —Å–≤—è–∑–∞–Ω—ã —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏, —É–ø–æ–º—è–Ω–∏ —ç—Ç–æ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ.\n"

            # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –Ω–æ–≤–æ—Å—Ç–µ–π
            if personalization_context:
                news_text += f"\n\nüéØ –ü–ï–†–°–û–ù–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:\n{personalization_context}\n"

            # –°–æ–∑–¥–∞–µ–º payload –¥–ª—è –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
            input_payload = {
                "category": category,
                "style_profile": personalized_style,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å—Ç–∏–ª—å
                "tone": personalized_tone,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–Ω
                "length": length,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª–∏–Ω—ã
                "audience": personalized_audience,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∞—É–¥–∏—Ç–æ—Ä–∏—é
                "news_text": news_text,
                "min_importance": 0.6,
                "min_credibility": 0.7,
            }

            # –í—ã—á–∏—Å–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤—ã–±–æ—Ä–∞ –ø–µ—Ä—Å–æ–Ω—ã
            urgency = 0.5  # Default
            complexity = 0.5  # Default
            if news_data:
                avg_importance = sum(item.get("importance", 0.5) for item in news_data) / len(news_data)
                urgency = min(avg_importance, 1.0)  # Use importance as urgency proxy
                complexity = 0.8 if len(news_data) > 5 else 0.4  # More news = more complex

            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–µ—Ä—Å–æ–Ω –∏ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π
                if self.config.use_personas and SUPER_JOURNALIST_V3_AVAILABLE:
                    from digests.prompts_v2 import build_prompt_with_persona

                    system_prompt, user_prompt = build_prompt_with_persona(
                        input_payload,
                        persona_id=None,  # Auto-select
                        subcategory=subcategory,
                        urgency=urgency,
                        complexity=complexity,
                        news_count=len(news_data),
                        avg_importance=avg_importance if news_data else 0.5,
                    )
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π –±–µ–∑ –ø–µ—Ä—Å–æ–Ω
                    from digests.prompts_v2 import build_prompt_with_subcategory

                    system_prompt, user_prompt = build_prompt_with_subcategory(input_payload, subcategory)

                final_prompt = f"{system_prompt}\n\n{user_prompt}"

                # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –Ω–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
                if len(final_prompt) > 8000:
                    logger.warning(f"Prompts_v2 prompt too large ({len(final_prompt)} chars), truncating")
                    final_prompt = final_prompt[:8000] + "\n\n[–¢–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è]"

                logger.info(f"Prompts_v2 final size: {len(final_prompt)} characters")
                return final_prompt
            except ImportError:
                # Fallback –Ω–∞ —Å—Ç–∞—Ä—É—é —Ñ—É–Ω–∫—Ü–∏—é
                system_prompt, user_prompt = build_prompt(input_payload)
                final_prompt = f"{system_prompt}\n\n{user_prompt}"
                if len(final_prompt) > 8000:
                    final_prompt = final_prompt[:8000] + "\n\n[–¢–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è]"
                return final_prompt
            except Exception as e:
                logger.warning(f"Failed to use prompts_v2, falling back to legacy: {e}")

        # Fallback –∫ —Å—Ç–∞—Ä–æ–π —Å–∏—Å—Ç–µ–º–µ
        logger.info(f"Using legacy prompts for style: {personalized_style}, category: {category}")

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏—è –∫ news_text –µ—Å–ª–∏ –µ—Å—Ç—å (–¥–ª—è legacy —Å–∏—Å—Ç–µ–º—ã —Ç–æ–∂–µ)
        if events:
            events_text = "\n\nüìÖ –ü–†–ï–î–°–¢–û–Ø–©–ò–ï –°–û–ë–´–¢–ò–Ø:\n" + "\n".join(
                [f"‚Ä¢ {e['title']} ({e['date']})" for e in events[:3]]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è legacy
            )
            news_text += events_text

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ legacy —Å–∏—Å—Ç–µ–º–∞ —ç—Ç–æ—Ç —Å—Ç–∏–ª—å
        fallback_style = personalized_style
        if personalized_style not in LEGACY_PROMPTS:
            logger.warning(f"Legacy system doesn't support style '{personalized_style}', falling back to 'analytical'")
            fallback_style = "analytical"

        formatted_prompt = get_prompt_for_category(fallback_style, category)

        # –°–æ–∑–¥–∞–µ–º –±–ª–æ–∫ —Å—Å—ã–ª–æ–∫
        links_block = "\n".join([f"- {item['source']}: {item.get('link', 'No link')}" for item in news_data])

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º—Ç —Å –¥–∞–Ω–Ω—ã–º–∏
        final_prompt = formatted_prompt.replace("{text_block}", news_text).replace("{links_block}", links_block)

        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        if len(final_prompt) > 8000:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–∏–π —Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞
            logger.warning(f"Prompt too large ({len(final_prompt)} chars), truncating to 8000")
            final_prompt = final_prompt[:8000] + "\n\n[–¢–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è]"

        logger.info(f"Final prompt size: {len(final_prompt)} characters")
        return final_prompt

    def _build_fallback_digest(self, news_items: List[NewsItem]) -> str:
        """
        Build simple digest without AI when OpenAI is not available.

        Args:
            news_items: List of news items

        Returns:
            Simple formatted digest
        """
        digest_parts = ["üì∞ <b>–î–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π</b>\n"]

        for i, item in enumerate(news_items, 1):
            date_str = format_date(item.published_at) if item.published_at else "‚Äî"
            credibility = f"{item.credibility:.1f}" if item.credibility else "‚Äî"
            importance = f"{item.importance:.1f}" if item.importance else "‚Äî"

            digest_parts.append(
                f"<b>{i}. {item.title}</b>\n"
                f"üìÖ {date_str} | üîó {item.source or 'Unknown'}\n"
                f"üìä –î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å: {credibility} | –í–∞–∂–Ω–æ—Å—Ç—å: {importance}\n"
            )

        # Add fallback section
        if self.config.include_fallback:
            digest_parts.append(self._get_fallback_section())

        return "\n".join(digest_parts)

    def _build_empty_digest(self) -> str:
        """Build digest for empty news list."""
        return "üì∞ <b>–î–∞–π–¥–∂–µ—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π</b>\n\n–°–µ–≥–æ–¥–Ω—è –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç."

    def _get_fallback_section(self) -> str:
        """Get standard fallback section."""
        return """
<b>–ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ:</b>
‚Äî –°–æ–±—ã—Ç–∏—è –≤–ª–∏—è—é—Ç –Ω–∞ —Ä—ã–Ω–æ–∫ –∏ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏
‚Äî –í–∞–∂–Ω–æ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä–µ—à–µ–Ω–∏–π
‚Äî –ü–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω–∏–º–∞—Ç—å —Ç—Ä–µ–Ω–¥—ã –∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
"""


# Convenience functions for backward compatibility
async def generate_ai_digest(news_items: List[NewsItem], style: str = "analytical", max_items: int = 8) -> str:
    """
    Generate AI digest from news items.

    Args:
        news_items: List of NewsItem objects
        style: Digest style
        max_items: Maximum number of items to include

    Returns:
        Generated digest text
    """
    config = DigestConfig(max_items=max_items)
    service = DigestAIService(config)
    return await service.build_digest(news_items, style)


def generate_fallback_digest(news_items: List[NewsItem], max_items: int = 8) -> str:
    """
    Generate fallback digest without AI.

    Args:
        news_items: List of NewsItem objects
        max_items: Maximum number of items to include

    Returns:
        Generated digest text
    """
    config = DigestConfig(max_items=max_items)
    service = DigestAIService(config)
    return service._build_fallback_digest(news_items[:max_items])
