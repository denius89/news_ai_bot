"""
RAG (Retrieval-Augmented Generation) —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
"""

import json
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
import re
from collections import Counter

logger = logging.getLogger(__name__)


class DigestRAGSystem:
    """RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤."""

    def __init__(self, samples_file: str = "data/digest_training/samples.json"):
        self.samples_file = Path(samples_file)
        self.samples = []
        self._load_samples()

    def _load_samples(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –¥–∞–π–¥–∂–µ—Å—Ç–æ–≤."""
        try:
            if self.samples_file.exists():
                with open(self.samples_file, "r", encoding="utf-8") as f:
                    self.samples = json.load(f)
                logger.info(f"Loaded {len(self.samples)} digest samples")
            else:
                logger.warning(f"Samples file not found: {self.samples_file}")
                self.samples = []
        except Exception as e:
            logger.error(f"Failed to load samples: {e}")
            self.samples = []

    def reload_samples(self):
        """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∏–∑ —Ñ–∞–π–ª–∞."""
        logger.info("Reloading samples from file...")
        self._load_samples()

    def _calculate_relevance_score(
        self,
        sample: Dict[str, Any],
        target_category: str,
        target_subcategory: Optional[str] = None,
        target_style: str = "analytical",
        news_keywords: List[str] = None,
    ) -> float:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ—Ä–∞ –∫ —Ü–µ–ª–µ–≤—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º."""

        score = 0.0

        # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (+30)
        if sample.get("category") == target_category:
            score += 30.0

        # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (+25)
        if target_subcategory and sample.get("subcategory") == target_subcategory:
            score += 25.0

        # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ç–∏–ª—è (+20)
        if sample.get("style") == target_style:
            score += 20.0

        # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –Ω–æ–≤–æ—Å—Ç—è—Ö
        if news_keywords and sample.get("digest"):
            digest_text = sample["digest"].lower()
            keyword_matches = sum(1 for keyword in news_keywords if keyword.lower() in digest_text)
            score += keyword_matches * 5.0

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É (–≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
        source_bonus = {
            "The Bell": 5.0,
            "Bloomberg": 5.0,
            "Reuters": 4.0,
            "CoinDesk": 3.0,
            "TechCrunch": 4.0,
            "WSJ": 5.0,
        }
        source = sample.get("source", "").lower()
        for high_quality_source, bonus in source_bonus.items():
            if high_quality_source.lower() in source:
                score += bonus
                break

        # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω—É (–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–∞–π–¥–∂–µ—Å—Ç—ã)
        word_count = sample.get("word_count", 0)
        if 300 <= word_count <= 800:  # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
            score += 10.0
        elif 200 <= word_count < 300:
            score += 5.0

        return score

    def find_relevant_samples(
        self,
        category: str,
        subcategory: Optional[str] = None,
        style: str = "analytical",
        news_items: Optional[List[Any]] = None,
        max_samples: int = 3,
    ) -> List[Dict[str, Any]]:
        """
        –ù–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

        Args:
            category: –¶–µ–ª–µ–≤–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
            subcategory: –¶–µ–ª–µ–≤–∞—è –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è
            style: –°—Ç–∏–ª—å –¥–∞–π–¥–∂–µ—Å—Ç–∞
            news_items: –ù–æ–≤–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            max_samples: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –æ—Ü–µ–Ω–∫–∞–º–∏
        """

        if not self.samples:
            logger.warning("No samples available for RAG")
            return []

        # –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π
        news_keywords = []
        if news_items:
            for item in news_items:
                if hasattr(item, "title") and item.title:
                    # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                    words = re.findall(r"\b\w+\b", item.title.lower())
                    news_keywords.extend([w for w in words if len(w) > 3])

        # –†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
        scored_samples = []

        for sample in self.samples:
            score = self._calculate_relevance_score(
                sample=sample,
                target_category=category,
                target_subcategory=subcategory,
                target_style=style,
                news_keywords=news_keywords,
            )

            if score > 0:  # –¢–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
                scored_samples.append({"sample": sample, "score": score})

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        scored_samples.sort(key=lambda x: x["score"], reverse=True)

        # –í–µ—Ä–Ω—É—Ç—å —Ç–æ–ø –ø—Ä–∏–º–µ—Ä—ã
        result = scored_samples[:max_samples]

        if result:
            logger.info(
                f"Found {len(result)} relevant samples for {category}/{subcategory} with scores: {[r['score'] for r in result]}"
            )

        return result

    def create_rag_context(self, relevant_samples: List[Dict[str, Any]], max_context_length: int = 2000) -> str:
        """
        –°–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞.

        Args:
            relevant_samples: –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã —Å –æ—Ü–µ–Ω–∫–∞–º–∏
            max_context_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Returns:
            –°—Ç—Ä–æ–∫–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        """

        if not relevant_samples:
            return ""

        context_parts = []
        context_parts.append("üìö –ü–†–ò–ú–ï–†–´ –í–´–°–û–ö–û–ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–• –î–ê–ô–î–ñ–ï–°–¢–û–í:\n")

        for i, item in enumerate(relevant_samples, 1):
            sample = item["sample"]
            score = item["score"]

            context_part = f"""
–ü–†–ò–ú–ï–† {i} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.1f}):
–ò—Å—Ç–æ—á–Ω–∏–∫: {sample.get('source', 'Unknown')}
–°—Ç–∏–ª—å: {sample.get('style', 'analytical')}
–°–ª–æ–≤: {sample.get('word_count', 0)}

{sample.get('digest', '')}
"""

            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏ –æ–±—â–∞—è –¥–ª–∏–Ω–∞ –ª–∏–º–∏—Ç
            if len("\n".join(context_parts) + context_part) > max_context_length:
                break

            context_parts.append(context_part)

        context_parts.append(
            "\nüí° –ò–°–ü–û–õ–¨–ó–£–ô –≠–¢–ò –ü–†–ò–ú–ï–†–´ –ö–ê–ö –û–†–ò–ï–ù–¢–ò–† –¥–ª—è —Å—Ç–∏–ª—è, —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–≤–æ–µ–≥–æ –¥–∞–π–¥–∂–µ—Å—Ç–∞."
        )

        return "\n".join(context_parts)

    def get_style_guidance(
        self, category: str, subcategory: Optional[str] = None, style: str = "analytical"
    ) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç–∏–ª—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–º–µ—Ä–æ–≤.

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ —Å—Ç–∏–ª—é
        """

        relevant_samples = self.find_relevant_samples(
            category=category, subcategory=subcategory, style=style, max_samples=5
        )

        if not relevant_samples:
            return {"avg_word_count": 500, "common_phrases": [], "structure_pattern": "general"}

        # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã
        word_counts = [item["sample"].get("word_count", 0) for item in relevant_samples]
        avg_word_count = sum(word_counts) / len(word_counts) if word_counts else 500

        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        structures = []
        for item in relevant_samples:
            digest = item["sample"].get("digest", "")
            if "##" in digest or "<h2>" in digest:
                structures.append("sectioned")
            elif len(digest.split("\n\n")) > 3:
                structures.append("paragraphs")
            else:
                structures.append("single")

        structure_pattern = Counter(structures).most_common(1)[0][0] if structures else "paragraphs"

        # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ (–ø—Ä–æ—Å—Ç–æ–π)
        common_phrases = []
        for item in relevant_samples:
            digest = item["sample"].get("digest", "")
            # –ò–∑–≤–ª–µ—á—å –ø–µ—Ä–≤—ã–µ —Å–ª–æ–≤–∞ –∞–±–∑–∞—Ü–µ–≤ –∫–∞–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É—é—â–∏–µ —Ñ—Ä–∞–∑—ã
            paragraphs = digest.split("\n\n")
            for para in paragraphs[:2]:  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ –¥–≤–∞ –∞–±–∑–∞—Ü–∞
                first_sentence = para.split(".")[0] if "." in para else para[:100]
                if len(first_sentence) > 20:
                    common_phrases.append(first_sentence.strip())

        return {
            "avg_word_count": int(avg_word_count),
            "common_phrases": common_phrases[:3],
            "structure_pattern": structure_pattern,
            "sample_count": len(relevant_samples),
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä RAG —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
_global_rag_system = None


def _get_rag_system():
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä RAG —Å–∏—Å—Ç–µ–º—ã —Å –ª–µ–Ω–∏–≤–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π."""
    global _global_rag_system
    if _global_rag_system is None:
        _global_rag_system = DigestRAGSystem()
        logger.info(f"Initialized global RAG system with {len(_global_rag_system.samples)} samples")
    return _global_rag_system


# Convenience functions
def get_rag_context(
    category: str,
    subcategory: Optional[str] = None,
    style: str = "analytical",
    news_items: Optional[List[Any]] = None,
    max_samples: int = 3,
) -> str:
    """–ü–æ–ª—É—á–∏—Ç—å RAG –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞."""

    rag_system = _get_rag_system()
    # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ñ–∞–π–ª –∏–∑–º–µ–Ω–∏–ª—Å—è (TODO: –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É mtime)
    try:
        rag_system.reload_samples()
    except Exception as e:
        logger.warning(f"RAG reload failed, using cached: {e}")

    relevant_samples = rag_system.find_relevant_samples(
        category=category, subcategory=subcategory, style=style, news_items=news_items, max_samples=max_samples
    )

    return rag_system.create_rag_context(relevant_samples)


def get_style_recommendations(
    category: str, subcategory: Optional[str] = None, style: str = "analytical"
) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç–∏–ª—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–º–µ—Ä–æ–≤."""

    rag_system = DigestRAGSystem()
    # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    rag_system.reload_samples()

    return rag_system.get_style_guidance(category=category, subcategory=subcategory, style=style)
